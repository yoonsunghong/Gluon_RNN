{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/60/f685fb2cfb3088736bafbc9bdbb455327bdc8906b606da9c9a81bae1c81e/torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (676.9MB)\n",
      "\u001b[K     |████████████████████████████████| 676.9MB 110kB/s  eta 0:00:01     |█████                           | 103.5MB 66.7MB/s eta 0:00:09\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from torch) (1.14.6)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, rnn, utils as gutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  146M  100  146M    0     0  20.0M      0  0:00:07  0:00:07 --:--:-- 24.5M\n"
     ]
    }
   ],
   "source": [
    "!curl -Lo infersent1.pkl https://dl.fbaipublicfiles.com/senteval/infersent/infersent1.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../dataset/GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0   315    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 2075M  100 2075M    0     0  4502k      0  0:07:52  0:07:52 --:--:-- 10.2M    0  0:13:56  0:05:43  0:08:13 1259k 876M    0     0  2480k      0  0:14:16  0:06:01  0:08:15 1042k\n",
      "Archive:  ../dataset/GloVe/glove.840B.300d.zip\n",
      "  inflating: glove.840B.300d.txt     \n"
     ]
    }
   ],
   "source": [
    "!curl -Lo ../dataset/GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "!unzip ../dataset/GloVe/glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "from models import InferSent\n",
    "model_version = 1\n",
    "MODEL_PATH = \"../encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep it on CPU or put it on GPU\n",
    "use_cuda = True\n",
    "model = model.cuda() if use_cuda else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
    "W2V_PATH = 'glove.840B.300d.txt'\n",
    "model.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "df = pd.read_csv('tokenized_tweets_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_full'] = df[\"filtered_text\"].apply(lambda x: \" \".join(ast.literal_eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"times\"] = pd.to_datetime((df['time']))\n",
    "df[\"year\"] = df[\"times\"].apply(lambda x: x.year)\n",
    "df = df[df[\"year\"]>2017].copy()\n",
    "df.drop([\"Unnamed: 0\", \"times\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>user</th>\n",
       "      <th>party</th>\n",
       "      <th>filtered_w_stop_text</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>text_full</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1115311440989184000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Get well soon, Assemblyman. @SeanMRyan149 http...</td>\n",
       "      <td>2019-04-08 17:52:23</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['get', 'well', 'soon', 'assemblyman']</td>\n",
       "      <td>['get', 'well', 'soon', 'assemblyman']</td>\n",
       "      <td>get well soon assemblyman</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1115298202092298241</td>\n",
       "      <td>133.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>This report is deeply alarming and goes agains...</td>\n",
       "      <td>2019-04-08 16:59:47</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['this', 'report', 'is', 'deeply', 'alarming',...</td>\n",
       "      <td>['report', 'deeply', 'alarming', 'goes', 'ever...</td>\n",
       "      <td>report deeply alarming goes everything stand a...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1114890416913092608</td>\n",
       "      <td>96.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>One of the big wins in the budget was the pass...</td>\n",
       "      <td>2019-04-07 13:59:23</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['one', 'of', 'the', 'big', 'wins', 'in', 'the...</td>\n",
       "      <td>['one', 'big', 'wins', 'budget', 'passage', 's...</td>\n",
       "      <td>one big wins budget passage senator jose peral...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1114667816941162497</td>\n",
       "      <td>169.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>We have to act quickly to fight climate change...</td>\n",
       "      <td>2019-04-06 23:14:51</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['we', 'have', 'to', 'act', 'quickly', 'to', '...</td>\n",
       "      <td>['act', 'quickly', 'fight', 'climate', 'change...</td>\n",
       "      <td>act quickly fight climate change part new york...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1114546662112923648</td>\n",
       "      <td>129.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>New York just make some important changes to o...</td>\n",
       "      <td>2019-04-06 15:13:26</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['new', 'york', 'just', 'make', 'some', 'impor...</td>\n",
       "      <td>['new', 'york', 'make', 'important', 'changes'...</td>\n",
       "      <td>new york make important changes discovery laws...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1114223726080536582</td>\n",
       "      <td>125.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>While Washington threatens the rights of New A...</td>\n",
       "      <td>2019-04-05 17:50:12</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['while', 'washington', 'threatens', 'the', 'r...</td>\n",
       "      <td>['washington', 'threatens', 'rights', 'new', '...</td>\n",
       "      <td>washington threatens rights new americans new ...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1114166661278576640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>RT @I_LOVE_NY: Craving an adventure? Vote in t...</td>\n",
       "      <td>2019-04-05 14:03:26</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['love', 'ny', 'craving', 'an', 'adventure', '...</td>\n",
       "      <td>['love', 'ny', 'craving', 'adventure', 'vote',...</td>\n",
       "      <td>love ny craving adventure vote final round ilo...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1113975587989798912</td>\n",
       "      <td>136.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>While the federal administration tries to buil...</td>\n",
       "      <td>2019-04-05 01:24:11</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['while', 'the', 'federal', 'administration', ...</td>\n",
       "      <td>['federal', 'administration', 'tries', 'build'...</td>\n",
       "      <td>federal administration tries build wall roll b...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1113902272801538048</td>\n",
       "      <td>132.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>In case you missed it, New York just codified ...</td>\n",
       "      <td>2019-04-04 20:32:51</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['in', 'case', 'you', 'missed', 'it', 'new', '...</td>\n",
       "      <td>['case', 'missed', 'new', 'york', 'codified', ...</td>\n",
       "      <td>case missed new york codified key provisions a...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1113863389397377024</td>\n",
       "      <td>79.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>A tragedy was averted thanks to the quick acti...</td>\n",
       "      <td>2019-04-04 17:58:21</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['a', 'tragedy', 'was', 'averted', 'thanks', '...</td>\n",
       "      <td>['tragedy', 'averted', 'thanks', 'quick', 'act...</td>\n",
       "      <td>tragedy averted thanks quick actions bystander...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1113591613467635713</td>\n",
       "      <td>196.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>In New York we just transformed our discovery ...</td>\n",
       "      <td>2019-04-03 23:58:24</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['in', 'new', 'york', 'we', 'just', 'transform...</td>\n",
       "      <td>['new', 'york', 'transformed', 'discovery', 'l...</td>\n",
       "      <td>new york transformed discovery law prosecutors...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1113526914982064129</td>\n",
       "      <td>868.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>Say goodbye to plastic bag pollution.\\n \\nMore...</td>\n",
       "      <td>2019-04-03 19:41:19</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['say', 'goodbye', 'to', 'plastic', 'bag', 'po...</td>\n",
       "      <td>['say', 'goodbye', 'plastic', 'bag', 'pollutio...</td>\n",
       "      <td>say goodbye plastic bag pollution states follo...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1113473324921905152</td>\n",
       "      <td>169.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Local journalism is the lifeblood of democracy...</td>\n",
       "      <td>2019-04-03 16:08:22</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['local', 'journalism', 'is', 'the', 'lifebloo...</td>\n",
       "      <td>['local', 'journalism', 'lifeblood', 'democrac...</td>\n",
       "      <td>local journalism lifeblood democracy healthy s...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1113444546539655168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>RT @NYSLabor: Whether you're just entering the...</td>\n",
       "      <td>2019-04-03 14:14:01</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['whether', 'you', 're', 'just', 'entering', '...</td>\n",
       "      <td>['whether', 'entering', 'workforce', 'ready', ...</td>\n",
       "      <td>whether entering workforce ready take next ste...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1113240943514222592</td>\n",
       "      <td>568.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>Our society needs to break our addiction to pl...</td>\n",
       "      <td>2019-04-03 00:44:58</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['our', 'society', 'needs', 'to', 'break', 'ou...</td>\n",
       "      <td>['society', 'needs', 'break', 'addiction', 'pl...</td>\n",
       "      <td>society needs break addiction plastic proud ba...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1113220079817244672</td>\n",
       "      <td>478.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>Plastic bags blight our land and clog our wate...</td>\n",
       "      <td>2019-04-02 23:22:04</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['plastic', 'bags', 'blight', 'our', 'land', '...</td>\n",
       "      <td>['plastic', 'bags', 'blight', 'land', 'clog', ...</td>\n",
       "      <td>plastic bags blight land clog waterways proud ...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1113185112559112193</td>\n",
       "      <td>71.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>I said we would not have a state budget that d...</td>\n",
       "      <td>2019-04-02 21:03:07</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['i', 'said', 'we', 'would', 'not', 'have', 'a...</td>\n",
       "      <td>['said', 'would', 'state', 'budget', 'include'...</td>\n",
       "      <td>said would state budget include permanent cap ...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1113162430924894209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>RT @NYSLabor: Today is Women’s #EqualPayDay, t...</td>\n",
       "      <td>2019-04-02 19:32:59</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['today', 'is', 'women', 'equalpayday', 'the',...</td>\n",
       "      <td>['today', 'women', 'equalpayday', 'day', 'wome...</td>\n",
       "      <td>today women equalpayday day women earnings cat...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1113140950476492800</td>\n",
       "      <td>105.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>We are making our criminal justice system more...</td>\n",
       "      <td>2019-04-02 18:07:38</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['we', 'are', 'making', 'our', 'criminal', 'ju...</td>\n",
       "      <td>['making', 'criminal', 'justice', 'system', 'f...</td>\n",
       "      <td>making criminal justice system fair ending cas...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1113110014842097667</td>\n",
       "      <td>151.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>Today, on #EqualPayDay, I once again call for ...</td>\n",
       "      <td>2019-04-02 16:04:42</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['today', 'on', 'equalpayday', 'i', 'once', 'a...</td>\n",
       "      <td>['today', 'equalpayday', 'call', 'passage', 's...</td>\n",
       "      <td>today equalpayday call passage salary history ...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1113083331384655872</td>\n",
       "      <td>523.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>\"Puerto Rico has already been scheduled to rec...</td>\n",
       "      <td>2019-04-02 14:18:40</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['puerto', 'rico', 'has', 'already', 'been', '...</td>\n",
       "      <td>['puerto', 'rico', 'already', 'scheduled', 're...</td>\n",
       "      <td>puerto rico already scheduled receive hurrican...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>1113069643042820097</td>\n",
       "      <td>53.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>We removed barriers to IVF and fertility cover...</td>\n",
       "      <td>2019-04-02 13:24:17</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['we', 'removed', 'barriers', 'to', 'ivf', 'an...</td>\n",
       "      <td>['removed', 'barriers', 'ivf', 'fertility', 'c...</td>\n",
       "      <td>removed barriers ivf fertility coverage decidi...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1113049044060966913</td>\n",
       "      <td>353.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>Here’s the truth @SenRickScott: Florida, gets ...</td>\n",
       "      <td>2019-04-02 12:02:26</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['here', 'the', 'truth', 'florida', 'gets', 'w...</td>\n",
       "      <td>['truth', 'florida', 'gets', 'welfare', 'peopl...</td>\n",
       "      <td>truth florida gets welfare people ny pay bills</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1112854115137282049</td>\n",
       "      <td>94.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>We just enacted landmark criminal justice refo...</td>\n",
       "      <td>2019-04-01 23:07:51</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['we', 'just', 'enacted', 'landmark', 'crimina...</td>\n",
       "      <td>['enacted', 'landmark', 'criminal', 'justice',...</td>\n",
       "      <td>enacted landmark criminal justice reforms comm...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>1112822484087259145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>RT @NYGovCuomo: We got it done! Here’s what’s ...</td>\n",
       "      <td>2019-04-01 21:02:10</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['we', 'got', 'it', 'done', 'here', 'what', 'i...</td>\n",
       "      <td>['got', 'done', 'year', 'budget']</td>\n",
       "      <td>got done year budget</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>1112795604957229056</td>\n",
       "      <td>119.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>.@NYSPolice Trooper Joshua Kaye is a hero. His...</td>\n",
       "      <td>2019-04-01 19:15:21</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['trooper', 'joshua', 'kaye', 'is', 'a', 'hero...</td>\n",
       "      <td>['trooper', 'joshua', 'kaye', 'hero', 'selfles...</td>\n",
       "      <td>trooper joshua kaye hero selflessness bravery ...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1112519060267433984</td>\n",
       "      <td>346.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>We got it done! Here’s what’s in this year’s b...</td>\n",
       "      <td>2019-04-01 00:56:28</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['we', 'got', 'it', 'done', 'here', 'what', 'i...</td>\n",
       "      <td>['got', 'done', 'year', 'budget']</td>\n",
       "      <td>got done year budget</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1112416632226435073</td>\n",
       "      <td>125.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>This #TransDayOfVisibility, we’re celebrating ...</td>\n",
       "      <td>2019-03-31 18:09:27</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['this', 'transdayofvisibility', 'we', 're', '...</td>\n",
       "      <td>['transdayofvisibility', 'celebrating', 'passa...</td>\n",
       "      <td>transdayofvisibility celebrating passage genda...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1112348228815671296</td>\n",
       "      <td>216.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>I said New York would not see a budget without...</td>\n",
       "      <td>2019-03-31 13:37:38</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['i', 'said', 'new', 'york', 'would', 'not', '...</td>\n",
       "      <td>['said', 'new', 'york', 'would', 'see', 'budge...</td>\n",
       "      <td>said new york would see budget without permane...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1111720667995619328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>RT @NYGov: Support your favorite state park, h...</td>\n",
       "      <td>2019-03-29 20:03:56</td>\n",
       "      <td>NYGovCuomo</td>\n",
       "      <td>1</td>\n",
       "      <td>['support', 'your', 'favorite', 'state', 'park...</td>\n",
       "      <td>['support', 'favorite', 'state', 'park', 'hist...</td>\n",
       "      <td>support favorite state park historic site publ...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259884</th>\n",
       "      <td>1259884</td>\n",
       "      <td>951163954851713025</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>About to speak on the #Senate floor with some ...</td>\n",
       "      <td>2018-01-10 18:48:53</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['about', 'to', 'speak', 'on', 'the', 'senate'...</td>\n",
       "      <td>['speak', 'senate', 'floor', 'colleagues', 'hi...</td>\n",
       "      <td>speak senate floor colleagues highlight positi...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259885</th>\n",
       "      <td>1259885</td>\n",
       "      <td>951105247077961734</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I'm \"ferry\" excited to join #SenateEPW today t...</td>\n",
       "      <td>2018-01-10 14:55:36</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['i', 'ferry', 'excited', 'to', 'join', 'senat...</td>\n",
       "      <td>['ferry', 'excited', 'join', 'senateepw', 'tod...</td>\n",
       "      <td>ferry excited join senateepw today talk boat n...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259886</th>\n",
       "      <td>1259886</td>\n",
       "      <td>950917535809069056</td>\n",
       "      <td>62.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Congrats to my friend &amp;amp; colleague @rep_ste...</td>\n",
       "      <td>2018-01-10 02:29:42</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['congrats', 'to', 'my', 'friend', 'colleague'...</td>\n",
       "      <td>['congrats', 'friend', 'colleague', 'stevewoma...</td>\n",
       "      <td>congrats friend colleague stevewomack recommen...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259887</th>\n",
       "      <td>1259887</td>\n",
       "      <td>950854376964415488</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Thanks for stopping by. Enjoyed discussing the...</td>\n",
       "      <td>2018-01-09 22:18:44</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['thanks', 'for', 'stopping', 'by', 'enjoyed',...</td>\n",
       "      <td>['thanks', 'stopping', 'enjoyed', 'discussing'...</td>\n",
       "      <td>thanks stopping enjoyed discussing great work</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259888</th>\n",
       "      <td>1259888</td>\n",
       "      <td>950829779363815424</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Today is #LawEnforcementAppreciationDay. Thank...</td>\n",
       "      <td>2018-01-09 20:40:59</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['today', 'is', 'lawenforcementappreciationday...</td>\n",
       "      <td>['today', 'lawenforcementappreciationday', 'th...</td>\n",
       "      <td>today lawenforcementappreciationday thankful s...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259889</th>\n",
       "      <td>1259889</td>\n",
       "      <td>950795849918971905</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I had a great conversation on @CSPAN's Washing...</td>\n",
       "      <td>2018-01-09 18:26:10</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['i', 'had', 'a', 'great', 'conversation', 'on...</td>\n",
       "      <td>['great', 'conversation', 'washington', 'journ...</td>\n",
       "      <td>great conversation washington journal morning ...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259890</th>\n",
       "      <td>1259890</td>\n",
       "      <td>950719105979514880</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I'll be joining @cspanwj at 7:40am CT to talk ...</td>\n",
       "      <td>2018-01-09 13:21:13</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['i', 'll', 'be', 'joining', 'at', 'am', 'ct',...</td>\n",
       "      <td>['joining', 'ct', 'talk', 'efforts', 'reach', ...</td>\n",
       "      <td>joining ct talk efforts reach agreement govern...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259891</th>\n",
       "      <td>1259891</td>\n",
       "      <td>950507961964810242</td>\n",
       "      <td>53.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>I will continue working with @POTUS to help ha...</td>\n",
       "      <td>2018-01-08 23:22:12</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['i', 'will', 'continue', 'working', 'with', '...</td>\n",
       "      <td>['continue', 'working', 'help', 'hardworking',...</td>\n",
       "      <td>continue working help hardworking farmers ranc...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259892</th>\n",
       "      <td>1259892</td>\n",
       "      <td>950464911649509378</td>\n",
       "      <td>43.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Good luck and Godspeed to the @usairforce airm...</td>\n",
       "      <td>2018-01-08 20:31:08</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['good', 'luck', 'and', 'godspeed', 'to', 'the...</td>\n",
       "      <td>['good', 'luck', 'godspeed', 'airmen', 'little...</td>\n",
       "      <td>good luck godspeed airmen littlerock air force...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259893</th>\n",
       "      <td>1259893</td>\n",
       "      <td>950455493406978049</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Looking forward to continuing this progress in...</td>\n",
       "      <td>2018-01-08 19:53:42</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['looking', 'forward', 'to', 'continuing', 'th...</td>\n",
       "      <td>['looking', 'forward', 'continuing', 'progress...</td>\n",
       "      <td>looking forward continuing progress thank lead...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259894</th>\n",
       "      <td>1259894</td>\n",
       "      <td>950448886073749505</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>READ my latest column about why January is Nat...</td>\n",
       "      <td>2018-01-08 19:27:27</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['read', 'my', 'latest', 'column', 'about', 'w...</td>\n",
       "      <td>['read', 'latest', 'column', 'january', 'natio...</td>\n",
       "      <td>read latest column january national slavery hu...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259895</th>\n",
       "      <td>1259895</td>\n",
       "      <td>950062109752209410</td>\n",
       "      <td>50.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Congrats to the @EAST_Armorel students whose c...</td>\n",
       "      <td>2018-01-07 17:50:32</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['congrats', 'to', 'the', 'armorel', 'students...</td>\n",
       "      <td>['congrats', 'armorel', 'students', 'whose', '...</td>\n",
       "      <td>congrats armorel students whose creativity des...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259896</th>\n",
       "      <td>1259896</td>\n",
       "      <td>950032423185780736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>RT @ArFB: ArFB President Randy Veach waving th...</td>\n",
       "      <td>2018-01-07 15:52:35</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['arfb', 'president', 'randy', 'veach', 'wavin...</td>\n",
       "      <td>['arfb', 'president', 'randy', 'veach', 'wavin...</td>\n",
       "      <td>arfb president randy veach waving arkansas fla...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259897</th>\n",
       "      <td>1259897</td>\n",
       "      <td>949715477886009345</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>More great news for American workers as @taxre...</td>\n",
       "      <td>2018-01-06 18:53:09</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['more', 'great', 'news', 'for', 'american', '...</td>\n",
       "      <td>['great', 'news', 'american', 'workers', 'crun...</td>\n",
       "      <td>great news american workers crunching numbers ...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259898</th>\n",
       "      <td>1259898</td>\n",
       "      <td>949402875280707584</td>\n",
       "      <td>372.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Enjoyed getting to stop by @UArkansas and meet...</td>\n",
       "      <td>2018-01-05 22:10:59</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['enjoyed', 'getting', 'to', 'stop', 'by', 'an...</td>\n",
       "      <td>['enjoyed', 'getting', 'stop', 'meet', 'new', ...</td>\n",
       "      <td>enjoyed getting stop meet new ad looking forwa...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259899</th>\n",
       "      <td>1259899</td>\n",
       "      <td>949371215654797312</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great to visit with @RogersFire &amp;amp; @Springd...</td>\n",
       "      <td>2018-01-05 20:05:10</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['great', 'to', 'visit', 'with', 'fire', 'to',...</td>\n",
       "      <td>['great', 'visit', 'fire', 'discuss', 'fire', ...</td>\n",
       "      <td>great visit fire discuss fire grants get updat...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259900</th>\n",
       "      <td>1259900</td>\n",
       "      <td>949337986063392768</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Constituent service is one of my top prioritie...</td>\n",
       "      <td>2018-01-05 17:53:08</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['constituent', 'service', 'is', 'one', 'of', ...</td>\n",
       "      <td>['constituent', 'service', 'one', 'top', 'prio...</td>\n",
       "      <td>constituent service one top priorities u senat...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259901</th>\n",
       "      <td>1259901</td>\n",
       "      <td>949307898894671872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>RT @SecretaryAcosta: The American workforce sh...</td>\n",
       "      <td>2018-01-05 15:53:35</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['the', 'american', 'workforce', 'showed', 'im...</td>\n",
       "      <td>['american', 'workforce', 'showed', 'impressiv...</td>\n",
       "      <td>american workforce showed impressive strength ...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259902</th>\n",
       "      <td>1259902</td>\n",
       "      <td>949058832851812352</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Congratulations to the Griffin family on their...</td>\n",
       "      <td>2018-01-04 23:23:53</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['congratulations', 'to', 'the', 'griffin', 'f...</td>\n",
       "      <td>['congratulations', 'griffin', 'family', 'new'...</td>\n",
       "      <td>congratulations griffin family new bundle joy</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259903</th>\n",
       "      <td>1259903</td>\n",
       "      <td>949057159244984323</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Congratulations to Ron Chastain, Gary Churchil...</td>\n",
       "      <td>2018-01-04 23:17:14</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['congratulations', 'to', 'ron', 'chastain', '...</td>\n",
       "      <td>['congratulations', 'ron', 'chastain', 'gary',...</td>\n",
       "      <td>congratulations ron chastain gary churchill sa...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259904</th>\n",
       "      <td>1259904</td>\n",
       "      <td>949035052813443077</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Pleased w/ the Administration's proposal to gi...</td>\n",
       "      <td>2018-01-04 21:49:23</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['pleased', 'the', 'administration', 'proposal...</td>\n",
       "      <td>['pleased', 'w', 'administration', 'proposal',...</td>\n",
       "      <td>pleased w administration proposal give small b...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259905</th>\n",
       "      <td>1259905</td>\n",
       "      <td>948976570152124421</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>ICYMI: Fort Smith is celebrating its #bicenten...</td>\n",
       "      <td>2018-01-04 17:57:00</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['icymi', 'fort', 'smith', 'is', 'celebrating'...</td>\n",
       "      <td>['icymi', 'fort', 'smith', 'celebrating', 'bic...</td>\n",
       "      <td>icymi fort smith celebrating bicentennial year...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259906</th>\n",
       "      <td>1259906</td>\n",
       "      <td>948956508250955778</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I’m excited for the Fort Smith community as it...</td>\n",
       "      <td>2018-01-04 16:37:16</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['i', 'excited', 'for', 'the', 'fort', 'smith'...</td>\n",
       "      <td>['excited', 'fort', 'smith', 'community', 'com...</td>\n",
       "      <td>excited fort smith community comes together re...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259907</th>\n",
       "      <td>1259907</td>\n",
       "      <td>948956058642468870</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>As home to a growing university, new medical s...</td>\n",
       "      <td>2018-01-04 16:35:29</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['as', 'home', 'to', 'a', 'growing', 'universi...</td>\n",
       "      <td>['home', 'growing', 'university', 'new', 'medi...</td>\n",
       "      <td>home growing university new medical school vib...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259908</th>\n",
       "      <td>1259908</td>\n",
       "      <td>948953859497644032</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Congratulations to Fort Smith, #Arkansas on #2...</td>\n",
       "      <td>2018-01-04 16:26:45</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['congratulations', 'to', 'fort', 'smith', 'ar...</td>\n",
       "      <td>['congratulations', 'fort', 'smith', 'arkansas...</td>\n",
       "      <td>congratulations fort smith arkansas years spea...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259909</th>\n",
       "      <td>1259909</td>\n",
       "      <td>948593972062113794</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>If you missed it when it aired before the holi...</td>\n",
       "      <td>2018-01-03 16:36:41</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['if', 'you', 'missed', 'it', 'when', 'it', 'a...</td>\n",
       "      <td>['missed', 'aired', 'holidays', 'visited', 'st...</td>\n",
       "      <td>missed aired holidays visited steve barnes tax...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259910</th>\n",
       "      <td>1259910</td>\n",
       "      <td>948582744614932482</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>More positive news for American workers as a r...</td>\n",
       "      <td>2018-01-03 15:52:04</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['more', 'positive', 'news', 'for', 'american'...</td>\n",
       "      <td>['positive', 'news', 'american', 'workers', 'r...</td>\n",
       "      <td>positive news american workers result taxrefor...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259911</th>\n",
       "      <td>1259911</td>\n",
       "      <td>948304587982860289</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The enormous impact of @senorrinhatch’s tirele...</td>\n",
       "      <td>2018-01-02 21:26:47</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['the', 'enormous', 'impact', 'of', 'tireless'...</td>\n",
       "      <td>['enormous', 'impact', 'tireless', 'work', 'th...</td>\n",
       "      <td>enormous impact tireless work throughout tenur...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259912</th>\n",
       "      <td>1259912</td>\n",
       "      <td>948290263990046720</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Our economy made a lot of positive strides in ...</td>\n",
       "      <td>2018-01-02 20:29:51</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['our', 'economy', 'made', 'a', 'lot', 'of', '...</td>\n",
       "      <td>['economy', 'made', 'lot', 'positive', 'stride...</td>\n",
       "      <td>economy made lot positive strides new jobs une...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259913</th>\n",
       "      <td>1259913</td>\n",
       "      <td>947708994860097536</td>\n",
       "      <td>43.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>#HappyNewYear! Hope your 2018 is happy, health...</td>\n",
       "      <td>2018-01-01 06:00:06</td>\n",
       "      <td>JohnBoozman</td>\n",
       "      <td>-1</td>\n",
       "      <td>['happynewyear', 'hope', 'your', 'is', 'happy'...</td>\n",
       "      <td>['happynewyear', 'hope', 'happy', 'healthy', '...</td>\n",
       "      <td>happynewyear hope happy healthy bright</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517664 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.1                   id  favorite_count  retweet_count  \\\n",
       "0                   0  1115311440989184000            12.0            2.0   \n",
       "1                   1  1115298202092298241           133.0           39.0   \n",
       "2                   2  1114890416913092608            96.0           29.0   \n",
       "3                   3  1114667816941162497           169.0           75.0   \n",
       "4                   4  1114546662112923648           129.0           40.0   \n",
       "5                   5  1114223726080536582           125.0           41.0   \n",
       "6                   6  1114166661278576640             0.0           10.0   \n",
       "7                   7  1113975587989798912           136.0           30.0   \n",
       "8                   8  1113902272801538048           132.0           31.0   \n",
       "9                   9  1113863389397377024            79.0           15.0   \n",
       "10                 10  1113591613467635713           196.0           64.0   \n",
       "11                 11  1113526914982064129           868.0          225.0   \n",
       "12                 12  1113473324921905152           169.0           36.0   \n",
       "13                 13  1113444546539655168             0.0           15.0   \n",
       "14                 14  1113240943514222592           568.0          143.0   \n",
       "15                 15  1113220079817244672           478.0          112.0   \n",
       "16                 16  1113185112559112193            71.0           17.0   \n",
       "17                 17  1113162430924894209             0.0           15.0   \n",
       "18                 18  1113140950476492800           105.0           44.0   \n",
       "19                 19  1113110014842097667           151.0           41.0   \n",
       "20                 20  1113083331384655872           523.0          162.0   \n",
       "21                 21  1113069643042820097            53.0           16.0   \n",
       "22                 22  1113049044060966913           353.0          154.0   \n",
       "23                 23  1112854115137282049            94.0           26.0   \n",
       "24                 24  1112822484087259145             0.0          101.0   \n",
       "25                 25  1112795604957229056           119.0           29.0   \n",
       "26                 26  1112519060267433984           346.0          101.0   \n",
       "27                 27  1112416632226435073           125.0           39.0   \n",
       "28                 28  1112348228815671296           216.0           59.0   \n",
       "29                 29  1111720667995619328             0.0           16.0   \n",
       "...               ...                  ...             ...            ...   \n",
       "1259884       1259884   951163954851713025            16.0            3.0   \n",
       "1259885       1259885   951105247077961734             9.0            3.0   \n",
       "1259886       1259886   950917535809069056            62.0           12.0   \n",
       "1259887       1259887   950854376964415488             1.0            2.0   \n",
       "1259888       1259888   950829779363815424            12.0            6.0   \n",
       "1259889       1259889   950795849918971905             8.0            4.0   \n",
       "1259890       1259890   950719105979514880             4.0            5.0   \n",
       "1259891       1259891   950507961964810242            53.0           17.0   \n",
       "1259892       1259892   950464911649509378            43.0           18.0   \n",
       "1259893       1259893   950455493406978049             5.0            5.0   \n",
       "1259894       1259894   950448886073749505            16.0           11.0   \n",
       "1259895       1259895   950062109752209410            50.0           19.0   \n",
       "1259896       1259896   950032423185780736             0.0            5.0   \n",
       "1259897       1259897   949715477886009345            30.0            7.0   \n",
       "1259898       1259898   949402875280707584           372.0           35.0   \n",
       "1259899       1259899   949371215654797312            11.0            4.0   \n",
       "1259900       1259900   949337986063392768            20.0            5.0   \n",
       "1259901       1259901   949307898894671872             0.0          212.0   \n",
       "1259902       1259902   949058832851812352            28.0            3.0   \n",
       "1259903       1259903   949057159244984323            10.0            5.0   \n",
       "1259904       1259904   949035052813443077            12.0           11.0   \n",
       "1259905       1259905   948976570152124421             4.0            4.0   \n",
       "1259906       1259906   948956508250955778             9.0            4.0   \n",
       "1259907       1259907   948956058642468870             4.0            3.0   \n",
       "1259908       1259908   948953859497644032            11.0            4.0   \n",
       "1259909       1259909   948593972062113794             9.0            4.0   \n",
       "1259910       1259910   948582744614932482            16.0            9.0   \n",
       "1259911       1259911   948304587982860289            20.0            4.0   \n",
       "1259912       1259912   948290263990046720            13.0            7.0   \n",
       "1259913       1259913   947708994860097536            43.0           10.0   \n",
       "\n",
       "                                                      text  \\\n",
       "0        Get well soon, Assemblyman. @SeanMRyan149 http...   \n",
       "1        This report is deeply alarming and goes agains...   \n",
       "2        One of the big wins in the budget was the pass...   \n",
       "3        We have to act quickly to fight climate change...   \n",
       "4        New York just make some important changes to o...   \n",
       "5        While Washington threatens the rights of New A...   \n",
       "6        RT @I_LOVE_NY: Craving an adventure? Vote in t...   \n",
       "7        While the federal administration tries to buil...   \n",
       "8        In case you missed it, New York just codified ...   \n",
       "9        A tragedy was averted thanks to the quick acti...   \n",
       "10       In New York we just transformed our discovery ...   \n",
       "11       Say goodbye to plastic bag pollution.\\n \\nMore...   \n",
       "12       Local journalism is the lifeblood of democracy...   \n",
       "13       RT @NYSLabor: Whether you're just entering the...   \n",
       "14       Our society needs to break our addiction to pl...   \n",
       "15       Plastic bags blight our land and clog our wate...   \n",
       "16       I said we would not have a state budget that d...   \n",
       "17       RT @NYSLabor: Today is Women’s #EqualPayDay, t...   \n",
       "18       We are making our criminal justice system more...   \n",
       "19       Today, on #EqualPayDay, I once again call for ...   \n",
       "20       \"Puerto Rico has already been scheduled to rec...   \n",
       "21       We removed barriers to IVF and fertility cover...   \n",
       "22       Here’s the truth @SenRickScott: Florida, gets ...   \n",
       "23       We just enacted landmark criminal justice refo...   \n",
       "24       RT @NYGovCuomo: We got it done! Here’s what’s ...   \n",
       "25       .@NYSPolice Trooper Joshua Kaye is a hero. His...   \n",
       "26       We got it done! Here’s what’s in this year’s b...   \n",
       "27       This #TransDayOfVisibility, we’re celebrating ...   \n",
       "28       I said New York would not see a budget without...   \n",
       "29       RT @NYGov: Support your favorite state park, h...   \n",
       "...                                                    ...   \n",
       "1259884  About to speak on the #Senate floor with some ...   \n",
       "1259885  I'm \"ferry\" excited to join #SenateEPW today t...   \n",
       "1259886  Congrats to my friend &amp; colleague @rep_ste...   \n",
       "1259887  Thanks for stopping by. Enjoyed discussing the...   \n",
       "1259888  Today is #LawEnforcementAppreciationDay. Thank...   \n",
       "1259889  I had a great conversation on @CSPAN's Washing...   \n",
       "1259890  I'll be joining @cspanwj at 7:40am CT to talk ...   \n",
       "1259891  I will continue working with @POTUS to help ha...   \n",
       "1259892  Good luck and Godspeed to the @usairforce airm...   \n",
       "1259893  Looking forward to continuing this progress in...   \n",
       "1259894  READ my latest column about why January is Nat...   \n",
       "1259895  Congrats to the @EAST_Armorel students whose c...   \n",
       "1259896  RT @ArFB: ArFB President Randy Veach waving th...   \n",
       "1259897  More great news for American workers as @taxre...   \n",
       "1259898  Enjoyed getting to stop by @UArkansas and meet...   \n",
       "1259899  Great to visit with @RogersFire &amp; @Springd...   \n",
       "1259900  Constituent service is one of my top prioritie...   \n",
       "1259901  RT @SecretaryAcosta: The American workforce sh...   \n",
       "1259902  Congratulations to the Griffin family on their...   \n",
       "1259903  Congratulations to Ron Chastain, Gary Churchil...   \n",
       "1259904  Pleased w/ the Administration's proposal to gi...   \n",
       "1259905  ICYMI: Fort Smith is celebrating its #bicenten...   \n",
       "1259906  I’m excited for the Fort Smith community as it...   \n",
       "1259907  As home to a growing university, new medical s...   \n",
       "1259908  Congratulations to Fort Smith, #Arkansas on #2...   \n",
       "1259909  If you missed it when it aired before the holi...   \n",
       "1259910  More positive news for American workers as a r...   \n",
       "1259911  The enormous impact of @senorrinhatch’s tirele...   \n",
       "1259912  Our economy made a lot of positive strides in ...   \n",
       "1259913  #HappyNewYear! Hope your 2018 is happy, health...   \n",
       "\n",
       "                        time         user  party  \\\n",
       "0        2019-04-08 17:52:23   NYGovCuomo      1   \n",
       "1        2019-04-08 16:59:47   NYGovCuomo      1   \n",
       "2        2019-04-07 13:59:23   NYGovCuomo      1   \n",
       "3        2019-04-06 23:14:51   NYGovCuomo      1   \n",
       "4        2019-04-06 15:13:26   NYGovCuomo      1   \n",
       "5        2019-04-05 17:50:12   NYGovCuomo      1   \n",
       "6        2019-04-05 14:03:26   NYGovCuomo      1   \n",
       "7        2019-04-05 01:24:11   NYGovCuomo      1   \n",
       "8        2019-04-04 20:32:51   NYGovCuomo      1   \n",
       "9        2019-04-04 17:58:21   NYGovCuomo      1   \n",
       "10       2019-04-03 23:58:24   NYGovCuomo      1   \n",
       "11       2019-04-03 19:41:19   NYGovCuomo      1   \n",
       "12       2019-04-03 16:08:22   NYGovCuomo      1   \n",
       "13       2019-04-03 14:14:01   NYGovCuomo      1   \n",
       "14       2019-04-03 00:44:58   NYGovCuomo      1   \n",
       "15       2019-04-02 23:22:04   NYGovCuomo      1   \n",
       "16       2019-04-02 21:03:07   NYGovCuomo      1   \n",
       "17       2019-04-02 19:32:59   NYGovCuomo      1   \n",
       "18       2019-04-02 18:07:38   NYGovCuomo      1   \n",
       "19       2019-04-02 16:04:42   NYGovCuomo      1   \n",
       "20       2019-04-02 14:18:40   NYGovCuomo      1   \n",
       "21       2019-04-02 13:24:17   NYGovCuomo      1   \n",
       "22       2019-04-02 12:02:26   NYGovCuomo      1   \n",
       "23       2019-04-01 23:07:51   NYGovCuomo      1   \n",
       "24       2019-04-01 21:02:10   NYGovCuomo      1   \n",
       "25       2019-04-01 19:15:21   NYGovCuomo      1   \n",
       "26       2019-04-01 00:56:28   NYGovCuomo      1   \n",
       "27       2019-03-31 18:09:27   NYGovCuomo      1   \n",
       "28       2019-03-31 13:37:38   NYGovCuomo      1   \n",
       "29       2019-03-29 20:03:56   NYGovCuomo      1   \n",
       "...                      ...          ...    ...   \n",
       "1259884  2018-01-10 18:48:53  JohnBoozman     -1   \n",
       "1259885  2018-01-10 14:55:36  JohnBoozman     -1   \n",
       "1259886  2018-01-10 02:29:42  JohnBoozman     -1   \n",
       "1259887  2018-01-09 22:18:44  JohnBoozman     -1   \n",
       "1259888  2018-01-09 20:40:59  JohnBoozman     -1   \n",
       "1259889  2018-01-09 18:26:10  JohnBoozman     -1   \n",
       "1259890  2018-01-09 13:21:13  JohnBoozman     -1   \n",
       "1259891  2018-01-08 23:22:12  JohnBoozman     -1   \n",
       "1259892  2018-01-08 20:31:08  JohnBoozman     -1   \n",
       "1259893  2018-01-08 19:53:42  JohnBoozman     -1   \n",
       "1259894  2018-01-08 19:27:27  JohnBoozman     -1   \n",
       "1259895  2018-01-07 17:50:32  JohnBoozman     -1   \n",
       "1259896  2018-01-07 15:52:35  JohnBoozman     -1   \n",
       "1259897  2018-01-06 18:53:09  JohnBoozman     -1   \n",
       "1259898  2018-01-05 22:10:59  JohnBoozman     -1   \n",
       "1259899  2018-01-05 20:05:10  JohnBoozman     -1   \n",
       "1259900  2018-01-05 17:53:08  JohnBoozman     -1   \n",
       "1259901  2018-01-05 15:53:35  JohnBoozman     -1   \n",
       "1259902  2018-01-04 23:23:53  JohnBoozman     -1   \n",
       "1259903  2018-01-04 23:17:14  JohnBoozman     -1   \n",
       "1259904  2018-01-04 21:49:23  JohnBoozman     -1   \n",
       "1259905  2018-01-04 17:57:00  JohnBoozman     -1   \n",
       "1259906  2018-01-04 16:37:16  JohnBoozman     -1   \n",
       "1259907  2018-01-04 16:35:29  JohnBoozman     -1   \n",
       "1259908  2018-01-04 16:26:45  JohnBoozman     -1   \n",
       "1259909  2018-01-03 16:36:41  JohnBoozman     -1   \n",
       "1259910  2018-01-03 15:52:04  JohnBoozman     -1   \n",
       "1259911  2018-01-02 21:26:47  JohnBoozman     -1   \n",
       "1259912  2018-01-02 20:29:51  JohnBoozman     -1   \n",
       "1259913  2018-01-01 06:00:06  JohnBoozman     -1   \n",
       "\n",
       "                                      filtered_w_stop_text  \\\n",
       "0                   ['get', 'well', 'soon', 'assemblyman']   \n",
       "1        ['this', 'report', 'is', 'deeply', 'alarming',...   \n",
       "2        ['one', 'of', 'the', 'big', 'wins', 'in', 'the...   \n",
       "3        ['we', 'have', 'to', 'act', 'quickly', 'to', '...   \n",
       "4        ['new', 'york', 'just', 'make', 'some', 'impor...   \n",
       "5        ['while', 'washington', 'threatens', 'the', 'r...   \n",
       "6        ['love', 'ny', 'craving', 'an', 'adventure', '...   \n",
       "7        ['while', 'the', 'federal', 'administration', ...   \n",
       "8        ['in', 'case', 'you', 'missed', 'it', 'new', '...   \n",
       "9        ['a', 'tragedy', 'was', 'averted', 'thanks', '...   \n",
       "10       ['in', 'new', 'york', 'we', 'just', 'transform...   \n",
       "11       ['say', 'goodbye', 'to', 'plastic', 'bag', 'po...   \n",
       "12       ['local', 'journalism', 'is', 'the', 'lifebloo...   \n",
       "13       ['whether', 'you', 're', 'just', 'entering', '...   \n",
       "14       ['our', 'society', 'needs', 'to', 'break', 'ou...   \n",
       "15       ['plastic', 'bags', 'blight', 'our', 'land', '...   \n",
       "16       ['i', 'said', 'we', 'would', 'not', 'have', 'a...   \n",
       "17       ['today', 'is', 'women', 'equalpayday', 'the',...   \n",
       "18       ['we', 'are', 'making', 'our', 'criminal', 'ju...   \n",
       "19       ['today', 'on', 'equalpayday', 'i', 'once', 'a...   \n",
       "20       ['puerto', 'rico', 'has', 'already', 'been', '...   \n",
       "21       ['we', 'removed', 'barriers', 'to', 'ivf', 'an...   \n",
       "22       ['here', 'the', 'truth', 'florida', 'gets', 'w...   \n",
       "23       ['we', 'just', 'enacted', 'landmark', 'crimina...   \n",
       "24       ['we', 'got', 'it', 'done', 'here', 'what', 'i...   \n",
       "25       ['trooper', 'joshua', 'kaye', 'is', 'a', 'hero...   \n",
       "26       ['we', 'got', 'it', 'done', 'here', 'what', 'i...   \n",
       "27       ['this', 'transdayofvisibility', 'we', 're', '...   \n",
       "28       ['i', 'said', 'new', 'york', 'would', 'not', '...   \n",
       "29       ['support', 'your', 'favorite', 'state', 'park...   \n",
       "...                                                    ...   \n",
       "1259884  ['about', 'to', 'speak', 'on', 'the', 'senate'...   \n",
       "1259885  ['i', 'ferry', 'excited', 'to', 'join', 'senat...   \n",
       "1259886  ['congrats', 'to', 'my', 'friend', 'colleague'...   \n",
       "1259887  ['thanks', 'for', 'stopping', 'by', 'enjoyed',...   \n",
       "1259888  ['today', 'is', 'lawenforcementappreciationday...   \n",
       "1259889  ['i', 'had', 'a', 'great', 'conversation', 'on...   \n",
       "1259890  ['i', 'll', 'be', 'joining', 'at', 'am', 'ct',...   \n",
       "1259891  ['i', 'will', 'continue', 'working', 'with', '...   \n",
       "1259892  ['good', 'luck', 'and', 'godspeed', 'to', 'the...   \n",
       "1259893  ['looking', 'forward', 'to', 'continuing', 'th...   \n",
       "1259894  ['read', 'my', 'latest', 'column', 'about', 'w...   \n",
       "1259895  ['congrats', 'to', 'the', 'armorel', 'students...   \n",
       "1259896  ['arfb', 'president', 'randy', 'veach', 'wavin...   \n",
       "1259897  ['more', 'great', 'news', 'for', 'american', '...   \n",
       "1259898  ['enjoyed', 'getting', 'to', 'stop', 'by', 'an...   \n",
       "1259899  ['great', 'to', 'visit', 'with', 'fire', 'to',...   \n",
       "1259900  ['constituent', 'service', 'is', 'one', 'of', ...   \n",
       "1259901  ['the', 'american', 'workforce', 'showed', 'im...   \n",
       "1259902  ['congratulations', 'to', 'the', 'griffin', 'f...   \n",
       "1259903  ['congratulations', 'to', 'ron', 'chastain', '...   \n",
       "1259904  ['pleased', 'the', 'administration', 'proposal...   \n",
       "1259905  ['icymi', 'fort', 'smith', 'is', 'celebrating'...   \n",
       "1259906  ['i', 'excited', 'for', 'the', 'fort', 'smith'...   \n",
       "1259907  ['as', 'home', 'to', 'a', 'growing', 'universi...   \n",
       "1259908  ['congratulations', 'to', 'fort', 'smith', 'ar...   \n",
       "1259909  ['if', 'you', 'missed', 'it', 'when', 'it', 'a...   \n",
       "1259910  ['more', 'positive', 'news', 'for', 'american'...   \n",
       "1259911  ['the', 'enormous', 'impact', 'of', 'tireless'...   \n",
       "1259912  ['our', 'economy', 'made', 'a', 'lot', 'of', '...   \n",
       "1259913  ['happynewyear', 'hope', 'your', 'is', 'happy'...   \n",
       "\n",
       "                                             filtered_text  \\\n",
       "0                   ['get', 'well', 'soon', 'assemblyman']   \n",
       "1        ['report', 'deeply', 'alarming', 'goes', 'ever...   \n",
       "2        ['one', 'big', 'wins', 'budget', 'passage', 's...   \n",
       "3        ['act', 'quickly', 'fight', 'climate', 'change...   \n",
       "4        ['new', 'york', 'make', 'important', 'changes'...   \n",
       "5        ['washington', 'threatens', 'rights', 'new', '...   \n",
       "6        ['love', 'ny', 'craving', 'adventure', 'vote',...   \n",
       "7        ['federal', 'administration', 'tries', 'build'...   \n",
       "8        ['case', 'missed', 'new', 'york', 'codified', ...   \n",
       "9        ['tragedy', 'averted', 'thanks', 'quick', 'act...   \n",
       "10       ['new', 'york', 'transformed', 'discovery', 'l...   \n",
       "11       ['say', 'goodbye', 'plastic', 'bag', 'pollutio...   \n",
       "12       ['local', 'journalism', 'lifeblood', 'democrac...   \n",
       "13       ['whether', 'entering', 'workforce', 'ready', ...   \n",
       "14       ['society', 'needs', 'break', 'addiction', 'pl...   \n",
       "15       ['plastic', 'bags', 'blight', 'land', 'clog', ...   \n",
       "16       ['said', 'would', 'state', 'budget', 'include'...   \n",
       "17       ['today', 'women', 'equalpayday', 'day', 'wome...   \n",
       "18       ['making', 'criminal', 'justice', 'system', 'f...   \n",
       "19       ['today', 'equalpayday', 'call', 'passage', 's...   \n",
       "20       ['puerto', 'rico', 'already', 'scheduled', 're...   \n",
       "21       ['removed', 'barriers', 'ivf', 'fertility', 'c...   \n",
       "22       ['truth', 'florida', 'gets', 'welfare', 'peopl...   \n",
       "23       ['enacted', 'landmark', 'criminal', 'justice',...   \n",
       "24                       ['got', 'done', 'year', 'budget']   \n",
       "25       ['trooper', 'joshua', 'kaye', 'hero', 'selfles...   \n",
       "26                       ['got', 'done', 'year', 'budget']   \n",
       "27       ['transdayofvisibility', 'celebrating', 'passa...   \n",
       "28       ['said', 'new', 'york', 'would', 'see', 'budge...   \n",
       "29       ['support', 'favorite', 'state', 'park', 'hist...   \n",
       "...                                                    ...   \n",
       "1259884  ['speak', 'senate', 'floor', 'colleagues', 'hi...   \n",
       "1259885  ['ferry', 'excited', 'join', 'senateepw', 'tod...   \n",
       "1259886  ['congrats', 'friend', 'colleague', 'stevewoma...   \n",
       "1259887  ['thanks', 'stopping', 'enjoyed', 'discussing'...   \n",
       "1259888  ['today', 'lawenforcementappreciationday', 'th...   \n",
       "1259889  ['great', 'conversation', 'washington', 'journ...   \n",
       "1259890  ['joining', 'ct', 'talk', 'efforts', 'reach', ...   \n",
       "1259891  ['continue', 'working', 'help', 'hardworking',...   \n",
       "1259892  ['good', 'luck', 'godspeed', 'airmen', 'little...   \n",
       "1259893  ['looking', 'forward', 'continuing', 'progress...   \n",
       "1259894  ['read', 'latest', 'column', 'january', 'natio...   \n",
       "1259895  ['congrats', 'armorel', 'students', 'whose', '...   \n",
       "1259896  ['arfb', 'president', 'randy', 'veach', 'wavin...   \n",
       "1259897  ['great', 'news', 'american', 'workers', 'crun...   \n",
       "1259898  ['enjoyed', 'getting', 'stop', 'meet', 'new', ...   \n",
       "1259899  ['great', 'visit', 'fire', 'discuss', 'fire', ...   \n",
       "1259900  ['constituent', 'service', 'one', 'top', 'prio...   \n",
       "1259901  ['american', 'workforce', 'showed', 'impressiv...   \n",
       "1259902  ['congratulations', 'griffin', 'family', 'new'...   \n",
       "1259903  ['congratulations', 'ron', 'chastain', 'gary',...   \n",
       "1259904  ['pleased', 'w', 'administration', 'proposal',...   \n",
       "1259905  ['icymi', 'fort', 'smith', 'celebrating', 'bic...   \n",
       "1259906  ['excited', 'fort', 'smith', 'community', 'com...   \n",
       "1259907  ['home', 'growing', 'university', 'new', 'medi...   \n",
       "1259908  ['congratulations', 'fort', 'smith', 'arkansas...   \n",
       "1259909  ['missed', 'aired', 'holidays', 'visited', 'st...   \n",
       "1259910  ['positive', 'news', 'american', 'workers', 'r...   \n",
       "1259911  ['enormous', 'impact', 'tireless', 'work', 'th...   \n",
       "1259912  ['economy', 'made', 'lot', 'positive', 'stride...   \n",
       "1259913  ['happynewyear', 'hope', 'happy', 'healthy', '...   \n",
       "\n",
       "                                                 text_full  year  \n",
       "0                                get well soon assemblyman  2019  \n",
       "1        report deeply alarming goes everything stand a...  2019  \n",
       "2        one big wins budget passage senator jose peral...  2019  \n",
       "3        act quickly fight climate change part new york...  2019  \n",
       "4        new york make important changes discovery laws...  2019  \n",
       "5        washington threatens rights new americans new ...  2019  \n",
       "6        love ny craving adventure vote final round ilo...  2019  \n",
       "7        federal administration tries build wall roll b...  2019  \n",
       "8        case missed new york codified key provisions a...  2019  \n",
       "9        tragedy averted thanks quick actions bystander...  2019  \n",
       "10       new york transformed discovery law prosecutors...  2019  \n",
       "11       say goodbye plastic bag pollution states follo...  2019  \n",
       "12       local journalism lifeblood democracy healthy s...  2019  \n",
       "13       whether entering workforce ready take next ste...  2019  \n",
       "14       society needs break addiction plastic proud ba...  2019  \n",
       "15       plastic bags blight land clog waterways proud ...  2019  \n",
       "16       said would state budget include permanent cap ...  2019  \n",
       "17       today women equalpayday day women earnings cat...  2019  \n",
       "18       making criminal justice system fair ending cas...  2019  \n",
       "19       today equalpayday call passage salary history ...  2019  \n",
       "20       puerto rico already scheduled receive hurrican...  2019  \n",
       "21       removed barriers ivf fertility coverage decidi...  2019  \n",
       "22          truth florida gets welfare people ny pay bills  2019  \n",
       "23       enacted landmark criminal justice reforms comm...  2019  \n",
       "24                                    got done year budget  2019  \n",
       "25       trooper joshua kaye hero selflessness bravery ...  2019  \n",
       "26                                    got done year budget  2019  \n",
       "27       transdayofvisibility celebrating passage genda...  2019  \n",
       "28       said new york would see budget without permane...  2019  \n",
       "29       support favorite state park historic site publ...  2019  \n",
       "...                                                    ...   ...  \n",
       "1259884  speak senate floor colleagues highlight positi...  2018  \n",
       "1259885  ferry excited join senateepw today talk boat n...  2018  \n",
       "1259886  congrats friend colleague stevewomack recommen...  2018  \n",
       "1259887      thanks stopping enjoyed discussing great work  2018  \n",
       "1259888  today lawenforcementappreciationday thankful s...  2018  \n",
       "1259889  great conversation washington journal morning ...  2018  \n",
       "1259890  joining ct talk efforts reach agreement govern...  2018  \n",
       "1259891  continue working help hardworking farmers ranc...  2018  \n",
       "1259892  good luck godspeed airmen littlerock air force...  2018  \n",
       "1259893  looking forward continuing progress thank lead...  2018  \n",
       "1259894  read latest column january national slavery hu...  2018  \n",
       "1259895  congrats armorel students whose creativity des...  2018  \n",
       "1259896  arfb president randy veach waving arkansas fla...  2018  \n",
       "1259897  great news american workers crunching numbers ...  2018  \n",
       "1259898  enjoyed getting stop meet new ad looking forwa...  2018  \n",
       "1259899  great visit fire discuss fire grants get updat...  2018  \n",
       "1259900  constituent service one top priorities u senat...  2018  \n",
       "1259901  american workforce showed impressive strength ...  2018  \n",
       "1259902      congratulations griffin family new bundle joy  2018  \n",
       "1259903  congratulations ron chastain gary churchill sa...  2018  \n",
       "1259904  pleased w administration proposal give small b...  2018  \n",
       "1259905  icymi fort smith celebrating bicentennial year...  2018  \n",
       "1259906  excited fort smith community comes together re...  2018  \n",
       "1259907  home growing university new medical school vib...  2018  \n",
       "1259908  congratulations fort smith arkansas years spea...  2018  \n",
       "1259909  missed aired holidays visited steve barnes tax...  2018  \n",
       "1259910  positive news american workers result taxrefor...  2018  \n",
       "1259911  enormous impact tireless work throughout tenur...  2018  \n",
       "1259912  economy made lot positive strides new jobs une...  2018  \n",
       "1259913             happynewyear hope happy healthy bright  2018  \n",
       "\n",
       "[517664 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/56/90178929712ce427ebad179f8dc46c8deef4e89d4c853092bee1efd57d05/nltk-3.4.1.zip (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 45.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from nltk) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/97/8a/10/d646015f33c525688e91986c4544c68019b19a473cb33d3b55\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 63389(/106951) words with w2v vectors\n",
      "Vocab size : 63389\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings of K most frequent words\n",
    "#model.build_vocab_k_words(K=100000)\n",
    "#once we have the sentences ready\n",
    "model.build_vocab(df['text_full'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu mode : >> 1000 sentences/s\n",
    "# cpu mode : ~100 sentences/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb words kept : 8632626/8887943 (97.1%)\n",
      "Speed : 955.8 sentences/s (gpu mode, bsize=64)\n",
      "nb sentences encoded : 517664\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(df['text_full'], verbose=True)\n",
    "print('nb sentences encoded : {0}'.format(len(embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = model.encode(sentences, bsize=128, tokenize=False, verbose=True)\n",
    "# print('nb sentences encoded : {0}'.format(len(embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = mx.gluon.data.dataset.ArrayDataset(embeddings, df['party'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['party'][df['party'] == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting d2l\n",
      "  Downloading https://files.pythonhosted.org/packages/64/5c/951071157d1dbb36e771a2ca38a38fee4d6af4951776ff44f7a2dc3d07f0/d2l-0.9.2-py3-none-any.whl\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from d2l) (2.2.2)\n",
      "Requirement already satisfied: jupyter in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from d2l) (1.0.0)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from d2l) (0.23.4)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from d2l) (1.14.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from matplotlib->d2l) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from matplotlib->d2l) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from matplotlib->d2l) (2.8.0)\n",
      "Requirement already satisfied: pytz in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from matplotlib->d2l) (2019.1)\n",
      "Requirement already satisfied: six>=1.10 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from matplotlib->d2l) (1.12.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from matplotlib->d2l) (1.1.0)\n",
      "Requirement already satisfied: qtconsole in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from jupyter->d2l) (4.4.3)\n",
      "Requirement already satisfied: notebook in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from jupyter->d2l) (5.7.8)\n",
      "Requirement already satisfied: nbconvert in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from jupyter->d2l) (5.5.0)\n",
      "Requirement already satisfied: ipykernel in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from jupyter->d2l) (5.1.0)\n",
      "Requirement already satisfied: jupyter-console in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from jupyter->d2l) (6.0.0)\n",
      "Requirement already satisfied: ipywidgets in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from jupyter->d2l) (7.4.2)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->d2l) (41.0.1)\n",
      "Requirement already satisfied: traitlets in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from qtconsole->jupyter->d2l) (4.3.2)\n",
      "Requirement already satisfied: ipython_genutils in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from qtconsole->jupyter->d2l) (0.2.0)\n",
      "Requirement already satisfied: jupyter_core in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from qtconsole->jupyter->d2l) (4.4.0)\n",
      "Requirement already satisfied: jupyter_client>=4.1 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from qtconsole->jupyter->d2l) (5.2.4)\n",
      "Requirement already satisfied: pygments in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from qtconsole->jupyter->d2l) (2.3.1)\n",
      "Requirement already satisfied: nbformat in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from notebook->jupyter->d2l) (4.4.0)\n",
      "Requirement already satisfied: tornado<7,>=4.1 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from notebook->jupyter->d2l) (6.0.2)\n",
      "Requirement already satisfied: Send2Trash in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from notebook->jupyter->d2l) (1.5.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from notebook->jupyter->d2l) (18.0.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from notebook->jupyter->d2l) (0.8.2)\n",
      "Requirement already satisfied: prometheus-client in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from notebook->jupyter->d2l) (0.6.0)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from notebook->jupyter->d2l) (2.10.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from nbconvert->jupyter->d2l) (0.3)\n",
      "Requirement already satisfied: bleach in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from nbconvert->jupyter->d2l) (3.1.0)\n",
      "Requirement already satisfied: testpath in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from nbconvert->jupyter->d2l) (0.4.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from nbconvert->jupyter->d2l) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from nbconvert->jupyter->d2l) (0.6.0)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from nbconvert->jupyter->d2l) (0.8.4)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from ipykernel->jupyter->d2l) (7.5.0)\n",
      "Requirement already satisfied: prompt_toolkit<2.1.0,>=2.0.0 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from jupyter-console->jupyter->d2l) (2.0.9)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from ipywidgets->jupyter->d2l) (3.4.2)\n",
      "Requirement already satisfied: decorator in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from traitlets->qtconsole->jupyter->d2l) (4.4.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from nbformat->notebook->jupyter->d2l) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from jinja2->notebook->jupyter->d2l) (1.1.1)\n",
      "Requirement already satisfied: webencodings in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from bleach->nbconvert->jupyter->d2l) (0.5.1)\n",
      "Requirement already satisfied: backcall in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l) (0.1.0)\n",
      "Requirement already satisfied: pickleshare in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l) (0.13.3)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l) (4.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from prompt_toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter->d2l) (0.1.7)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook->jupyter->d2l) (19.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook->jupyter->d2l) (0.14.11)\n",
      "Requirement already satisfied: parso>=0.3.0 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter->d2l) (0.4.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel->jupyter->d2l) (0.6.0)\n",
      "Installing collected packages: d2l\n",
      "Successfully installed d2l-0.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import d2l\n",
    "from mxnet import gluon, init\n",
    "from mxnet.gluon import loss as gloss, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting scikit-learn (from sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/82/c0de5839d613b82bddd088599ac0bbfbbbcbd8ca470680658352d2c435bd/scikit_learn-0.20.3-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4MB 46.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.13.3 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /home/ubuntu/miniconda3/envs/gluon/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.14.6)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-0.20.3 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, df['party'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon, init, nd, autograd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs, lr_period):\n",
    "    \"\"\"Train and evaluate a model.\"\"\"\n",
    "    print('training on', ctx)\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, m, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "             trainer.set_learning_rate(trainer.learning_rate * .5)\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            Xs, ys, batch_size = _get_batch(batch, ctx)\n",
    "            ls = []\n",
    "            with autograd.record():\n",
    "                y_hats = [net(X) for X in Xs]\n",
    "                ls = [loss(y_hat, y) for y_hat, y in zip(y_hats, ys)]\n",
    "            for l in ls:\n",
    "                l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_l_sum += sum([l.sum().asscalar() for l in ls])\n",
    "            n += sum([l.size for l in ls])\n",
    "            train_acc_sum += sum([(y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "                                  for y_hat, y in zip(y_hats, ys)])\n",
    "            m += sum([y.size for y in ys])\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / m, test_acc,\n",
    "                 time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_batch(batch, ctx):\n",
    "    \"\"\"Return features and labels on ctx.\"\"\"\n",
    "    features, labels = batch\n",
    "    if labels.dtype != features.dtype:\n",
    "        labels = labels.astype(features.dtype)\n",
    "    return (gutils.split_and_load(features, ctx),\n",
    "            gutils.split_and_load(labels, ctx), features.shape[0])\n",
    "\n",
    "def evaluate_accuracy(data_iter, net, ctx=[mx.cpu()]):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    acc_sum, n = nd.array([0]), 0\n",
    "    for batch in data_iter:\n",
    "        features, labels, _ = _get_batch(batch, ctx)\n",
    "        for X, y in zip(features, labels):\n",
    "            y = y.astype('float32')\n",
    "            acc_sum += (net(X).argmax(axis=1) == y).sum().copyto(mx.cpu())\n",
    "            n += y.size\n",
    "        acc_sum.wait_to_read()\n",
    "    return acc_sum.asscalar() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.HybridSequential()\n",
    "net.add(nn.Dense(2048, activation='relu'))\n",
    "net.add(gluon.nn.Dropout(0.25))\n",
    "net.add(nn.Dense(512, activation='relu'))\n",
    "net.add(nn.Dense(2))\n",
    "#     net.initialize(init.Normal(sigma=0.01))\n",
    "net.initialize(init.Xavier(), ctx=ctx)\n",
    "net.hybridize()\n",
    "\n",
    "# net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "batch_size = 128\n",
    "train_set = gdata.ArrayDataset(X_train, nd.array(y_train))\n",
    "test_set = gdata.ArrayDataset(X_test, nd.array(y_test))\n",
    "train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = gdata.DataLoader(test_set, batch_size)\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "num_epochs = 100\n",
    "# d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "#               None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.5910, train acc 0.678, test acc 0.690, time 21.6 sec\n",
      "epoch 2, loss 0.5418, train acc 0.716, test acc 0.712, time 21.5 sec\n",
      "epoch 3, loss 0.5159, train acc 0.735, test acc 0.739, time 21.5 sec\n",
      "epoch 4, loss 0.4980, train acc 0.747, test acc 0.675, time 21.5 sec\n",
      "epoch 5, loss 0.4831, train acc 0.756, test acc 0.716, time 21.5 sec\n",
      "epoch 6, loss 0.4707, train acc 0.765, test acc 0.765, time 21.5 sec\n",
      "epoch 7, loss 0.4586, train acc 0.773, test acc 0.753, time 21.5 sec\n",
      "epoch 8, loss 0.4481, train acc 0.780, test acc 0.772, time 21.6 sec\n",
      "epoch 9, loss 0.4369, train acc 0.786, test acc 0.775, time 21.8 sec\n",
      "epoch 10, loss 0.4265, train acc 0.792, test acc 0.773, time 21.7 sec\n",
      "epoch 11, loss 0.4138, train acc 0.799, test acc 0.745, time 21.6 sec\n",
      "epoch 12, loss 0.4041, train acc 0.806, test acc 0.772, time 21.6 sec\n",
      "epoch 13, loss 0.3947, train acc 0.811, test acc 0.740, time 21.5 sec\n",
      "epoch 14, loss 0.3859, train acc 0.816, test acc 0.782, time 21.5 sec\n",
      "epoch 15, loss 0.3748, train acc 0.823, test acc 0.765, time 21.6 sec\n",
      "epoch 16, loss 0.3653, train acc 0.828, test acc 0.783, time 21.5 sec\n",
      "epoch 17, loss 0.3554, train acc 0.833, test acc 0.783, time 21.4 sec\n",
      "epoch 18, loss 0.3455, train acc 0.838, test acc 0.777, time 21.5 sec\n",
      "epoch 19, loss 0.3364, train acc 0.844, test acc 0.779, time 21.5 sec\n",
      "epoch 20, loss 0.3265, train acc 0.849, test acc 0.781, time 21.5 sec\n",
      "epoch 21, loss 0.3110, train acc 0.857, test acc 0.741, time 21.5 sec\n",
      "epoch 22, loss 0.3023, train acc 0.861, test acc 0.730, time 21.5 sec\n",
      "epoch 23, loss 0.2944, train acc 0.866, test acc 0.785, time 21.6 sec\n",
      "epoch 24, loss 0.2843, train acc 0.872, test acc 0.784, time 21.7 sec\n",
      "epoch 25, loss 0.2755, train acc 0.876, test acc 0.732, time 21.6 sec\n",
      "epoch 26, loss 0.2666, train acc 0.880, test acc 0.788, time 21.6 sec\n",
      "epoch 27, loss 0.2574, train acc 0.885, test acc 0.753, time 21.6 sec\n",
      "epoch 28, loss 0.2500, train acc 0.889, test acc 0.776, time 21.6 sec\n",
      "epoch 29, loss 0.2398, train acc 0.894, test acc 0.771, time 21.6 sec\n",
      "epoch 30, loss 0.2320, train acc 0.898, test acc 0.785, time 21.6 sec\n",
      "epoch 31, loss 0.2168, train acc 0.905, test acc 0.763, time 21.6 sec\n",
      "epoch 32, loss 0.2093, train acc 0.908, test acc 0.793, time 21.6 sec\n",
      "epoch 33, loss 0.2016, train acc 0.912, test acc 0.777, time 21.6 sec\n",
      "epoch 34, loss 0.1942, train acc 0.915, test acc 0.778, time 21.5 sec\n",
      "epoch 35, loss 0.1879, train acc 0.919, test acc 0.771, time 21.8 sec\n",
      "epoch 36, loss 0.1818, train acc 0.922, test acc 0.788, time 21.6 sec\n",
      "epoch 37, loss 0.1752, train acc 0.925, test acc 0.743, time 21.5 sec\n",
      "epoch 38, loss 0.1696, train acc 0.927, test acc 0.785, time 21.6 sec\n",
      "epoch 39, loss 0.1629, train acc 0.931, test acc 0.753, time 21.6 sec\n",
      "epoch 40, loss 0.1576, train acc 0.933, test acc 0.794, time 21.5 sec\n",
      "epoch 41, loss 0.1435, train acc 0.939, test acc 0.791, time 21.5 sec\n",
      "epoch 42, loss 0.1381, train acc 0.942, test acc 0.788, time 21.5 sec\n",
      "epoch 43, loss 0.1340, train acc 0.944, test acc 0.773, time 21.7 sec\n",
      "epoch 44, loss 0.1300, train acc 0.945, test acc 0.790, time 21.7 sec\n",
      "epoch 45, loss 0.1261, train acc 0.948, test acc 0.787, time 21.5 sec\n",
      "epoch 46, loss 0.1212, train acc 0.950, test acc 0.786, time 21.6 sec\n",
      "epoch 47, loss 0.1176, train acc 0.951, test acc 0.775, time 21.5 sec\n",
      "epoch 48, loss 0.1145, train acc 0.953, test acc 0.776, time 21.5 sec\n",
      "epoch 49, loss 0.1109, train acc 0.954, test acc 0.772, time 21.6 sec\n",
      "epoch 50, loss 0.1077, train acc 0.956, test acc 0.786, time 21.6 sec\n",
      "epoch 51, loss 0.0965, train acc 0.960, test acc 0.780, time 21.5 sec\n",
      "epoch 52, loss 0.0950, train acc 0.961, test acc 0.793, time 21.6 sec\n",
      "epoch 53, loss 0.0926, train acc 0.962, test acc 0.788, time 21.6 sec\n",
      "epoch 54, loss 0.0905, train acc 0.962, test acc 0.786, time 21.5 sec\n",
      "epoch 55, loss 0.0862, train acc 0.965, test acc 0.794, time 21.5 sec\n",
      "epoch 56, loss 0.0847, train acc 0.965, test acc 0.765, time 21.5 sec\n",
      "epoch 57, loss 0.0831, train acc 0.966, test acc 0.795, time 21.5 sec\n",
      "epoch 58, loss 0.0798, train acc 0.967, test acc 0.769, time 21.6 sec\n",
      "epoch 59, loss 0.0793, train acc 0.968, test acc 0.795, time 21.4 sec\n",
      "epoch 60, loss 0.0766, train acc 0.969, test acc 0.785, time 21.5 sec\n",
      "epoch 61, loss 0.0686, train acc 0.972, test acc 0.788, time 21.5 sec\n",
      "epoch 62, loss 0.0681, train acc 0.972, test acc 0.796, time 21.5 sec\n",
      "epoch 63, loss 0.0657, train acc 0.973, test acc 0.754, time 21.5 sec\n",
      "epoch 64, loss 0.0655, train acc 0.973, test acc 0.793, time 21.6 sec\n",
      "epoch 65, loss 0.0626, train acc 0.974, test acc 0.793, time 21.5 sec\n",
      "epoch 66, loss 0.0626, train acc 0.974, test acc 0.797, time 21.7 sec\n",
      "epoch 67, loss 0.0619, train acc 0.975, test acc 0.775, time 21.6 sec\n",
      "epoch 68, loss 0.0604, train acc 0.975, test acc 0.797, time 21.5 sec\n",
      "epoch 69, loss 0.0587, train acc 0.976, test acc 0.796, time 21.6 sec\n",
      "epoch 70, loss 0.0578, train acc 0.976, test acc 0.792, time 21.5 sec\n",
      "epoch 71, loss 0.0536, train acc 0.978, test acc 0.797, time 21.5 sec\n",
      "epoch 72, loss 0.0521, train acc 0.979, test acc 0.796, time 21.6 sec\n",
      "epoch 73, loss 0.0509, train acc 0.979, test acc 0.795, time 21.5 sec\n",
      "epoch 74, loss 0.0499, train acc 0.979, test acc 0.797, time 21.6 sec\n",
      "epoch 75, loss 0.0497, train acc 0.980, test acc 0.788, time 21.5 sec\n",
      "epoch 76, loss 0.0484, train acc 0.980, test acc 0.785, time 21.5 sec\n",
      "epoch 77, loss 0.0476, train acc 0.980, test acc 0.790, time 21.6 sec\n",
      "epoch 78, loss 0.0474, train acc 0.980, test acc 0.797, time 21.5 sec\n",
      "epoch 79, loss 0.0467, train acc 0.981, test acc 0.791, time 21.6 sec\n",
      "epoch 80, loss 0.0469, train acc 0.980, test acc 0.796, time 21.5 sec\n",
      "epoch 81, loss 0.0415, train acc 0.983, test acc 0.794, time 21.6 sec\n",
      "epoch 82, loss 0.0418, train acc 0.983, test acc 0.799, time 21.5 sec\n",
      "epoch 83, loss 0.0403, train acc 0.983, test acc 0.796, time 21.5 sec\n",
      "epoch 84, loss 0.0400, train acc 0.983, test acc 0.790, time 21.6 sec\n",
      "epoch 85, loss 0.0401, train acc 0.983, test acc 0.795, time 21.6 sec\n",
      "epoch 86, loss 0.0387, train acc 0.984, test acc 0.798, time 21.6 sec\n",
      "epoch 87, loss 0.0399, train acc 0.983, test acc 0.797, time 21.5 sec\n",
      "epoch 88, loss 0.0387, train acc 0.984, test acc 0.794, time 21.6 sec\n",
      "epoch 89, loss 0.0390, train acc 0.984, test acc 0.797, time 21.5 sec\n",
      "epoch 90, loss 0.0375, train acc 0.984, test acc 0.797, time 21.5 sec\n",
      "epoch 91, loss 0.0348, train acc 0.985, test acc 0.798, time 21.5 sec\n",
      "epoch 92, loss 0.0352, train acc 0.985, test acc 0.799, time 21.7 sec\n",
      "epoch 93, loss 0.0341, train acc 0.986, test acc 0.788, time 21.5 sec\n",
      "epoch 94, loss 0.0337, train acc 0.986, test acc 0.797, time 21.5 sec\n",
      "epoch 95, loss 0.0329, train acc 0.986, test acc 0.800, time 21.5 sec\n",
      "epoch 96, loss 0.0328, train acc 0.986, test acc 0.798, time 21.7 sec\n",
      "epoch 97, loss 0.0331, train acc 0.986, test acc 0.797, time 21.6 sec\n",
      "epoch 98, loss 0.0327, train acc 0.986, test acc 0.796, time 21.6 sec\n",
      "epoch 99, loss 0.0325, train acc 0.986, test acc 0.799, time 21.6 sec\n",
      "epoch 100, loss 0.0325, train acc 0.986, test acc 0.800, time 21.6 sec\n"
     ]
    }
   ],
   "source": [
    "train(train_iter, test_iter, net, gloss.SoftmaxCrossEntropyLoss(), trainer, ctx, num_epochs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.HybridSequential()\n",
    "net.add(nn.Dense(1024, activation='relu'))\n",
    "net.add(gluon.nn.Dropout(0.25))\n",
    "net.add(nn.Dense(512, activation='relu'))\n",
    "net.add(nn.Dense(2))\n",
    "#     net.initialize(init.Normal(sigma=0.01))\n",
    "net.initialize(init.Xavier(), ctx=ctx)\n",
    "net.hybridize()\n",
    "\n",
    "# net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "batch_size = 128\n",
    "train_set = gdata.ArrayDataset(X_train, nd.array(y_train))\n",
    "test_set = gdata.ArrayDataset(X_test, nd.array(y_test))\n",
    "train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = gdata.DataLoader(test_set, batch_size)\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "num_epochs = 100\n",
    "# d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "#               None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.5914, train acc 0.677, test acc 0.704, time 15.8 sec\n",
      "epoch 2, loss 0.5435, train acc 0.716, test acc 0.725, time 15.7 sec\n",
      "epoch 3, loss 0.5185, train acc 0.733, test acc 0.738, time 15.8 sec\n",
      "epoch 4, loss 0.5005, train acc 0.745, test acc 0.731, time 15.8 sec\n",
      "epoch 5, loss 0.4864, train acc 0.754, test acc 0.757, time 15.8 sec\n",
      "epoch 6, loss 0.4743, train acc 0.762, test acc 0.688, time 15.6 sec\n",
      "epoch 7, loss 0.4630, train acc 0.770, test acc 0.769, time 15.6 sec\n",
      "epoch 8, loss 0.4530, train acc 0.776, test acc 0.757, time 15.6 sec\n",
      "epoch 9, loss 0.4433, train acc 0.783, test acc 0.764, time 15.7 sec\n",
      "epoch 10, loss 0.4340, train acc 0.788, test acc 0.757, time 15.9 sec\n",
      "epoch 11, loss 0.4223, train acc 0.795, test acc 0.751, time 15.7 sec\n",
      "epoch 12, loss 0.4135, train acc 0.800, test acc 0.780, time 15.7 sec\n",
      "epoch 13, loss 0.4058, train acc 0.805, test acc 0.776, time 15.7 sec\n",
      "epoch 14, loss 0.3971, train acc 0.810, test acc 0.779, time 15.7 sec\n",
      "epoch 15, loss 0.3894, train acc 0.815, test acc 0.749, time 15.6 sec\n",
      "epoch 16, loss 0.3800, train acc 0.819, test acc 0.774, time 15.6 sec\n",
      "epoch 17, loss 0.3723, train acc 0.824, test acc 0.771, time 15.6 sec\n",
      "epoch 18, loss 0.3642, train acc 0.829, test acc 0.784, time 15.6 sec\n",
      "epoch 19, loss 0.3557, train acc 0.833, test acc 0.689, time 15.6 sec\n",
      "epoch 20, loss 0.3472, train acc 0.838, test acc 0.788, time 15.7 sec\n",
      "epoch 21, loss 0.3351, train acc 0.845, test acc 0.788, time 15.8 sec\n",
      "epoch 22, loss 0.3266, train acc 0.849, test acc 0.785, time 15.6 sec\n",
      "epoch 23, loss 0.3184, train acc 0.854, test acc 0.776, time 15.6 sec\n",
      "epoch 24, loss 0.3111, train acc 0.858, test acc 0.791, time 15.6 sec\n",
      "epoch 25, loss 0.3036, train acc 0.861, test acc 0.788, time 15.6 sec\n",
      "epoch 26, loss 0.2962, train acc 0.866, test acc 0.729, time 15.6 sec\n",
      "epoch 27, loss 0.2884, train acc 0.870, test acc 0.784, time 15.7 sec\n",
      "epoch 28, loss 0.2803, train acc 0.873, test acc 0.767, time 15.7 sec\n",
      "epoch 29, loss 0.2729, train acc 0.877, test acc 0.780, time 15.7 sec\n",
      "epoch 30, loss 0.2660, train acc 0.881, test acc 0.791, time 15.6 sec\n",
      "epoch 31, loss 0.2513, train acc 0.888, test acc 0.773, time 15.6 sec\n",
      "epoch 32, loss 0.2472, train acc 0.890, test acc 0.766, time 15.6 sec\n",
      "epoch 33, loss 0.2376, train acc 0.895, test acc 0.748, time 15.8 sec\n",
      "epoch 34, loss 0.2314, train acc 0.898, test acc 0.787, time 15.7 sec\n",
      "epoch 35, loss 0.2264, train acc 0.900, test acc 0.787, time 15.8 sec\n",
      "epoch 36, loss 0.2205, train acc 0.903, test acc 0.784, time 15.6 sec\n",
      "epoch 37, loss 0.2143, train acc 0.906, test acc 0.784, time 15.7 sec\n",
      "epoch 38, loss 0.2084, train acc 0.909, test acc 0.787, time 15.7 sec\n",
      "epoch 39, loss 0.2014, train acc 0.912, test acc 0.774, time 15.7 sec\n",
      "epoch 40, loss 0.1973, train acc 0.915, test acc 0.790, time 15.7 sec\n",
      "epoch 41, loss 0.1842, train acc 0.920, test acc 0.784, time 15.7 sec\n",
      "epoch 42, loss 0.1797, train acc 0.922, test acc 0.779, time 15.6 sec\n",
      "epoch 43, loss 0.1748, train acc 0.925, test acc 0.793, time 15.7 sec\n",
      "epoch 44, loss 0.1716, train acc 0.927, test acc 0.784, time 15.6 sec\n",
      "epoch 45, loss 0.1661, train acc 0.929, test acc 0.776, time 15.6 sec\n",
      "epoch 46, loss 0.1620, train acc 0.931, test acc 0.789, time 15.6 sec\n",
      "epoch 47, loss 0.1564, train acc 0.934, test acc 0.789, time 15.6 sec\n",
      "epoch 48, loss 0.1537, train acc 0.935, test acc 0.782, time 15.7 sec\n",
      "epoch 49, loss 0.1493, train acc 0.937, test acc 0.777, time 15.7 sec\n",
      "epoch 50, loss 0.1455, train acc 0.939, test acc 0.792, time 15.7 sec\n",
      "epoch 51, loss 0.1341, train acc 0.944, test acc 0.785, time 15.7 sec\n",
      "epoch 52, loss 0.1311, train acc 0.945, test acc 0.788, time 15.7 sec\n",
      "epoch 53, loss 0.1281, train acc 0.947, test acc 0.775, time 15.6 sec\n",
      "epoch 54, loss 0.1247, train acc 0.948, test acc 0.774, time 15.7 sec\n",
      "epoch 55, loss 0.1225, train acc 0.949, test acc 0.785, time 15.7 sec\n",
      "epoch 56, loss 0.1198, train acc 0.950, test acc 0.740, time 15.7 sec\n",
      "epoch 57, loss 0.1166, train acc 0.951, test acc 0.790, time 15.7 sec\n",
      "epoch 58, loss 0.1135, train acc 0.953, test acc 0.782, time 15.7 sec\n",
      "epoch 59, loss 0.1115, train acc 0.953, test acc 0.794, time 15.7 sec\n",
      "epoch 60, loss 0.1095, train acc 0.955, test acc 0.788, time 15.7 sec\n",
      "epoch 61, loss 0.0999, train acc 0.959, test acc 0.769, time 15.8 sec\n",
      "epoch 62, loss 0.0994, train acc 0.958, test acc 0.786, time 15.8 sec\n",
      "epoch 63, loss 0.0982, train acc 0.959, test acc 0.786, time 15.7 sec\n",
      "epoch 64, loss 0.0946, train acc 0.961, test acc 0.786, time 15.6 sec\n",
      "epoch 65, loss 0.0929, train acc 0.962, test acc 0.791, time 15.6 sec\n",
      "epoch 66, loss 0.0931, train acc 0.961, test acc 0.797, time 15.7 sec\n",
      "epoch 67, loss 0.0900, train acc 0.963, test acc 0.793, time 15.7 sec\n",
      "epoch 68, loss 0.0876, train acc 0.964, test acc 0.794, time 15.7 sec\n",
      "epoch 69, loss 0.0866, train acc 0.965, test acc 0.792, time 15.7 sec\n",
      "epoch 70, loss 0.0855, train acc 0.965, test acc 0.789, time 15.7 sec\n",
      "epoch 71, loss 0.0776, train acc 0.968, test acc 0.792, time 15.6 sec\n",
      "epoch 72, loss 0.0773, train acc 0.968, test acc 0.792, time 15.7 sec\n",
      "epoch 73, loss 0.0762, train acc 0.969, test acc 0.776, time 15.7 sec\n",
      "epoch 74, loss 0.0749, train acc 0.969, test acc 0.793, time 15.7 sec\n",
      "epoch 75, loss 0.0728, train acc 0.970, test acc 0.796, time 15.7 sec\n",
      "epoch 76, loss 0.0727, train acc 0.970, test acc 0.788, time 15.7 sec\n",
      "epoch 77, loss 0.0719, train acc 0.970, test acc 0.791, time 15.6 sec\n",
      "epoch 78, loss 0.0707, train acc 0.971, test acc 0.796, time 15.7 sec\n",
      "epoch 79, loss 0.0702, train acc 0.971, test acc 0.789, time 15.7 sec\n",
      "epoch 80, loss 0.0690, train acc 0.972, test acc 0.792, time 15.7 sec\n",
      "epoch 81, loss 0.0621, train acc 0.975, test acc 0.794, time 15.7 sec\n",
      "epoch 82, loss 0.0625, train acc 0.974, test acc 0.792, time 15.8 sec\n",
      "epoch 83, loss 0.0624, train acc 0.974, test acc 0.797, time 15.7 sec\n",
      "epoch 84, loss 0.0610, train acc 0.975, test acc 0.796, time 15.7 sec\n",
      "epoch 85, loss 0.0603, train acc 0.975, test acc 0.794, time 15.7 sec\n",
      "epoch 86, loss 0.0599, train acc 0.975, test acc 0.797, time 15.7 sec\n",
      "epoch 87, loss 0.0588, train acc 0.976, test acc 0.797, time 15.7 sec\n",
      "epoch 88, loss 0.0575, train acc 0.976, test acc 0.795, time 15.7 sec\n",
      "epoch 89, loss 0.0571, train acc 0.977, test acc 0.798, time 15.7 sec\n",
      "epoch 90, loss 0.0564, train acc 0.977, test acc 0.795, time 15.8 sec\n",
      "epoch 91, loss 0.0524, train acc 0.978, test acc 0.796, time 15.7 sec\n",
      "epoch 92, loss 0.0524, train acc 0.978, test acc 0.798, time 15.7 sec\n",
      "epoch 93, loss 0.0525, train acc 0.978, test acc 0.798, time 15.7 sec\n",
      "epoch 94, loss 0.0513, train acc 0.979, test acc 0.796, time 15.7 sec\n",
      "epoch 95, loss 0.0508, train acc 0.979, test acc 0.790, time 15.6 sec\n",
      "epoch 96, loss 0.0509, train acc 0.979, test acc 0.791, time 15.6 sec\n",
      "epoch 97, loss 0.0490, train acc 0.980, test acc 0.797, time 15.6 sec\n",
      "epoch 98, loss 0.0495, train acc 0.980, test acc 0.796, time 15.7 sec\n",
      "epoch 99, loss 0.0497, train acc 0.979, test acc 0.796, time 15.7 sec\n",
      "epoch 100, loss 0.0482, train acc 0.980, test acc 0.796, time 15.8 sec\n"
     ]
    }
   ],
   "source": [
    "train(train_iter, test_iter, net, gloss.SoftmaxCrossEntropyLoss(), trainer, ctx, num_epochs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.HybridSequential()\n",
    "net.add(nn.Dense(512, activation='relu'))\n",
    "net.add(gluon.nn.Dropout(0.25))\n",
    "net.add(nn.Dense(2))\n",
    "#     net.initialize(init.Normal(sigma=0.01))\n",
    "net.initialize(init.Xavier(), ctx=ctx)\n",
    "net.hybridize()\n",
    "\n",
    "# net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "batch_size = 128\n",
    "train_set = gdata.ArrayDataset(X_train, nd.array(y_train))\n",
    "test_set = gdata.ArrayDataset(X_test, nd.array(y_test))\n",
    "train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = gdata.DataLoader(test_set, batch_size)\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "num_epochs = 100\n",
    "# d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "#               None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.5919, train acc 0.679, test acc 0.682, time 14.6 sec\n",
      "epoch 2, loss 0.5478, train acc 0.713, test acc 0.730, time 14.5 sec\n",
      "epoch 3, loss 0.5252, train acc 0.730, test acc 0.737, time 14.9 sec\n",
      "epoch 4, loss 0.5101, train acc 0.740, test acc 0.745, time 15.3 sec\n",
      "epoch 5, loss 0.4983, train acc 0.749, test acc 0.683, time 15.3 sec\n",
      "epoch 6, loss 0.4875, train acc 0.756, test acc 0.756, time 15.2 sec\n",
      "epoch 7, loss 0.4779, train acc 0.762, test acc 0.751, time 15.3 sec\n",
      "epoch 8, loss 0.4702, train acc 0.767, test acc 0.741, time 15.3 sec\n",
      "epoch 9, loss 0.4630, train acc 0.772, test acc 0.741, time 15.3 sec\n",
      "epoch 10, loss 0.4548, train acc 0.777, test acc 0.743, time 15.3 sec\n",
      "epoch 11, loss 0.4458, train acc 0.784, test acc 0.750, time 15.3 sec\n",
      "epoch 12, loss 0.4392, train acc 0.787, test acc 0.761, time 15.3 sec\n",
      "epoch 13, loss 0.4326, train acc 0.792, test acc 0.741, time 15.3 sec\n",
      "epoch 14, loss 0.4264, train acc 0.796, test acc 0.768, time 15.3 sec\n",
      "epoch 15, loss 0.4195, train acc 0.799, test acc 0.771, time 15.3 sec\n",
      "epoch 16, loss 0.4135, train acc 0.804, test acc 0.769, time 15.3 sec\n",
      "epoch 17, loss 0.4076, train acc 0.807, test acc 0.775, time 15.2 sec\n",
      "epoch 18, loss 0.4016, train acc 0.811, test acc 0.774, time 15.3 sec\n",
      "epoch 19, loss 0.3946, train acc 0.815, test acc 0.726, time 15.2 sec\n",
      "epoch 20, loss 0.3895, train acc 0.818, test acc 0.780, time 15.2 sec\n",
      "epoch 21, loss 0.3809, train acc 0.823, test acc 0.782, time 15.2 sec\n",
      "epoch 22, loss 0.3741, train acc 0.827, test acc 0.779, time 15.3 sec\n",
      "epoch 23, loss 0.3689, train acc 0.830, test acc 0.691, time 15.3 sec\n",
      "epoch 24, loss 0.3638, train acc 0.834, test acc 0.759, time 15.3 sec\n",
      "epoch 25, loss 0.3574, train acc 0.837, test acc 0.772, time 15.3 sec\n",
      "epoch 26, loss 0.3520, train acc 0.841, test acc 0.733, time 15.4 sec\n",
      "epoch 27, loss 0.3467, train acc 0.843, test acc 0.788, time 15.3 sec\n",
      "epoch 28, loss 0.3394, train acc 0.847, test acc 0.789, time 15.3 sec\n",
      "epoch 29, loss 0.3357, train acc 0.849, test acc 0.754, time 15.3 sec\n",
      "epoch 30, loss 0.3298, train acc 0.852, test acc 0.751, time 15.3 sec\n",
      "epoch 31, loss 0.3193, train acc 0.858, test acc 0.735, time 15.3 sec\n",
      "epoch 32, loss 0.3146, train acc 0.861, test acc 0.790, time 15.3 sec\n",
      "epoch 33, loss 0.3096, train acc 0.863, test acc 0.791, time 15.1 sec\n",
      "epoch 34, loss 0.3045, train acc 0.866, test acc 0.769, time 15.2 sec\n",
      "epoch 35, loss 0.2979, train acc 0.869, test acc 0.785, time 15.4 sec\n",
      "epoch 36, loss 0.2937, train acc 0.871, test acc 0.771, time 15.5 sec\n",
      "epoch 37, loss 0.2885, train acc 0.874, test acc 0.788, time 15.5 sec\n",
      "epoch 38, loss 0.2826, train acc 0.877, test acc 0.782, time 15.5 sec\n",
      "epoch 39, loss 0.2789, train acc 0.880, test acc 0.769, time 15.5 sec\n",
      "epoch 40, loss 0.2723, train acc 0.883, test acc 0.791, time 15.5 sec\n",
      "epoch 41, loss 0.2621, train acc 0.888, test acc 0.763, time 15.2 sec\n",
      "epoch 42, loss 0.2569, train acc 0.890, test acc 0.789, time 15.2 sec\n",
      "epoch 43, loss 0.2523, train acc 0.893, test acc 0.775, time 15.4 sec\n",
      "epoch 44, loss 0.2484, train acc 0.895, test acc 0.761, time 15.2 sec\n",
      "epoch 45, loss 0.2434, train acc 0.897, test acc 0.793, time 15.4 sec\n",
      "epoch 46, loss 0.2385, train acc 0.899, test acc 0.747, time 15.3 sec\n",
      "epoch 47, loss 0.2336, train acc 0.902, test acc 0.792, time 15.3 sec\n",
      "epoch 48, loss 0.2297, train acc 0.904, test acc 0.794, time 15.3 sec\n",
      "epoch 49, loss 0.2260, train acc 0.906, test acc 0.768, time 15.2 sec\n",
      "epoch 50, loss 0.2211, train acc 0.907, test acc 0.793, time 15.3 sec\n",
      "epoch 51, loss 0.2101, train acc 0.913, test acc 0.788, time 15.3 sec\n",
      "epoch 52, loss 0.2067, train acc 0.915, test acc 0.751, time 15.3 sec\n",
      "epoch 53, loss 0.2023, train acc 0.917, test acc 0.789, time 15.2 sec\n",
      "epoch 54, loss 0.1971, train acc 0.919, test acc 0.792, time 15.2 sec\n",
      "epoch 55, loss 0.1943, train acc 0.921, test acc 0.787, time 15.3 sec\n",
      "epoch 56, loss 0.1898, train acc 0.923, test acc 0.792, time 15.1 sec\n",
      "epoch 57, loss 0.1864, train acc 0.924, test acc 0.794, time 15.1 sec\n",
      "epoch 58, loss 0.1826, train acc 0.926, test acc 0.793, time 15.2 sec\n",
      "epoch 59, loss 0.1780, train acc 0.928, test acc 0.753, time 15.2 sec\n",
      "epoch 60, loss 0.1737, train acc 0.931, test acc 0.720, time 15.1 sec\n",
      "epoch 61, loss 0.1637, train acc 0.935, test acc 0.788, time 15.0 sec\n",
      "epoch 62, loss 0.1615, train acc 0.936, test acc 0.790, time 15.2 sec\n",
      "epoch 63, loss 0.1580, train acc 0.938, test acc 0.794, time 15.2 sec\n",
      "epoch 64, loss 0.1562, train acc 0.939, test acc 0.780, time 15.1 sec\n",
      "epoch 65, loss 0.1522, train acc 0.941, test acc 0.793, time 15.2 sec\n",
      "epoch 66, loss 0.1494, train acc 0.942, test acc 0.782, time 15.1 sec\n",
      "epoch 67, loss 0.1476, train acc 0.943, test acc 0.787, time 15.2 sec\n",
      "epoch 68, loss 0.1422, train acc 0.945, test acc 0.794, time 15.1 sec\n",
      "epoch 69, loss 0.1399, train acc 0.946, test acc 0.793, time 15.2 sec\n",
      "epoch 70, loss 0.1378, train acc 0.947, test acc 0.791, time 15.1 sec\n",
      "epoch 71, loss 0.1309, train acc 0.950, test acc 0.793, time 15.0 sec\n",
      "epoch 72, loss 0.1269, train acc 0.952, test acc 0.792, time 15.0 sec\n",
      "epoch 73, loss 0.1251, train acc 0.953, test acc 0.774, time 15.1 sec\n",
      "epoch 74, loss 0.1234, train acc 0.954, test acc 0.794, time 15.2 sec\n",
      "epoch 75, loss 0.1201, train acc 0.955, test acc 0.778, time 15.1 sec\n",
      "epoch 76, loss 0.1180, train acc 0.956, test acc 0.794, time 15.2 sec\n",
      "epoch 77, loss 0.1160, train acc 0.957, test acc 0.791, time 15.1 sec\n",
      "epoch 78, loss 0.1129, train acc 0.958, test acc 0.772, time 15.1 sec\n",
      "epoch 79, loss 0.1121, train acc 0.958, test acc 0.793, time 15.1 sec\n",
      "epoch 80, loss 0.1089, train acc 0.960, test acc 0.795, time 15.1 sec\n",
      "epoch 81, loss 0.1035, train acc 0.963, test acc 0.796, time 15.1 sec\n",
      "epoch 82, loss 0.1012, train acc 0.964, test acc 0.788, time 15.1 sec\n",
      "epoch 83, loss 0.0992, train acc 0.964, test acc 0.794, time 15.2 sec\n",
      "epoch 84, loss 0.0980, train acc 0.965, test acc 0.795, time 15.1 sec\n",
      "epoch 85, loss 0.0970, train acc 0.965, test acc 0.795, time 15.1 sec\n",
      "epoch 86, loss 0.0958, train acc 0.965, test acc 0.795, time 15.0 sec\n",
      "epoch 87, loss 0.0939, train acc 0.966, test acc 0.798, time 15.1 sec\n",
      "epoch 88, loss 0.0917, train acc 0.967, test acc 0.796, time 15.4 sec\n",
      "epoch 89, loss 0.0906, train acc 0.968, test acc 0.792, time 15.4 sec\n",
      "epoch 90, loss 0.0892, train acc 0.968, test acc 0.762, time 15.3 sec\n",
      "epoch 91, loss 0.0855, train acc 0.970, test acc 0.796, time 15.4 sec\n",
      "epoch 92, loss 0.0831, train acc 0.971, test acc 0.794, time 15.2 sec\n",
      "epoch 93, loss 0.0824, train acc 0.971, test acc 0.793, time 15.1 sec\n",
      "epoch 94, loss 0.0819, train acc 0.971, test acc 0.798, time 15.2 sec\n",
      "epoch 95, loss 0.0799, train acc 0.972, test acc 0.798, time 15.4 sec\n",
      "epoch 96, loss 0.0801, train acc 0.972, test acc 0.795, time 15.4 sec\n",
      "epoch 97, loss 0.0786, train acc 0.972, test acc 0.794, time 15.3 sec\n",
      "epoch 98, loss 0.0764, train acc 0.973, test acc 0.796, time 15.1 sec\n",
      "epoch 99, loss 0.0754, train acc 0.974, test acc 0.791, time 15.2 sec\n",
      "epoch 100, loss 0.0753, train acc 0.974, test acc 0.797, time 15.1 sec\n"
     ]
    }
   ],
   "source": [
    "train(train_iter, test_iter, net, gloss.SoftmaxCrossEntropyLoss(), trainer, ctx, num_epochs, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redefining Train, by adjusting lr decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs, lr_period):\n",
    "    \"\"\"Train and evaluate a model.\"\"\"\n",
    "    print('training on', ctx)\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, m, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "             trainer.set_learning_rate(trainer.learning_rate * .5)\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            Xs, ys, batch_size = _get_batch(batch, ctx)\n",
    "            ls = []\n",
    "            with autograd.record():\n",
    "                y_hats = [net(X) for X in Xs]\n",
    "                ls = [loss(y_hat, y) for y_hat, y in zip(y_hats, ys)]\n",
    "            for l in ls:\n",
    "                l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_l_sum += sum([l.sum().asscalar() for l in ls])\n",
    "            n += sum([l.size for l in ls])\n",
    "            train_acc_sum += sum([(y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "                                  for y_hat, y in zip(y_hats, ys)])\n",
    "            m += sum([y.size for y in ys])\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / m, test_acc,\n",
    "                 time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.HybridSequential()\n",
    "net.add(nn.Dense(512, activation='relu'))\n",
    "net.add(gluon.nn.Dropout(0.25))\n",
    "net.add(nn.Dense(2))\n",
    "#     net.initialize(init.Normal(sigma=0.01))\n",
    "net.initialize(init.Xavier(), ctx=ctx)\n",
    "net.hybridize()\n",
    "\n",
    "# net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "batch_size = 64\n",
    "train_set = gdata.ArrayDataset(X_train, nd.array(y_train))\n",
    "test_set = gdata.ArrayDataset(X_test, nd.array(y_test))\n",
    "train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = gdata.DataLoader(test_set, batch_size)\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "num_epochs = 100\n",
    "# d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "#               None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.5812, train acc 0.686, test acc 0.721, time 24.7 sec\n",
      "epoch 2, loss 0.5347, train acc 0.723, test acc 0.724, time 25.9 sec\n",
      "epoch 3, loss 0.5126, train acc 0.738, test acc 0.744, time 25.9 sec\n",
      "epoch 4, loss 0.4977, train acc 0.748, test acc 0.754, time 25.8 sec\n",
      "epoch 5, loss 0.4851, train acc 0.756, test acc 0.757, time 25.8 sec\n",
      "epoch 6, loss 0.4745, train acc 0.764, test acc 0.763, time 25.8 sec\n",
      "epoch 7, loss 0.4648, train acc 0.770, test acc 0.753, time 25.8 sec\n",
      "epoch 8, loss 0.4553, train acc 0.777, test acc 0.769, time 25.9 sec\n",
      "epoch 9, loss 0.4472, train acc 0.782, test acc 0.772, time 25.7 sec\n",
      "epoch 10, loss 0.4385, train acc 0.787, test acc 0.750, time 25.8 sec\n",
      "epoch 11, loss 0.4113, train acc 0.804, test acc 0.780, time 25.7 sec\n",
      "epoch 12, loss 0.4042, train acc 0.809, test acc 0.781, time 25.8 sec\n",
      "epoch 13, loss 0.3978, train acc 0.812, test acc 0.774, time 25.7 sec\n",
      "epoch 14, loss 0.3915, train acc 0.816, test acc 0.779, time 25.9 sec\n",
      "epoch 15, loss 0.3858, train acc 0.820, test acc 0.782, time 25.8 sec\n",
      "epoch 16, loss 0.3794, train acc 0.824, test acc 0.765, time 25.8 sec\n",
      "epoch 17, loss 0.3732, train acc 0.827, test acc 0.781, time 25.7 sec\n",
      "epoch 18, loss 0.3670, train acc 0.831, test acc 0.781, time 25.9 sec\n",
      "epoch 19, loss 0.3609, train acc 0.835, test acc 0.781, time 26.0 sec\n",
      "epoch 20, loss 0.3548, train acc 0.837, test acc 0.787, time 26.0 sec\n",
      "epoch 21, loss 0.3291, train acc 0.854, test acc 0.789, time 25.7 sec\n",
      "epoch 22, loss 0.3240, train acc 0.856, test acc 0.788, time 25.8 sec\n",
      "epoch 23, loss 0.3189, train acc 0.859, test acc 0.791, time 25.8 sec\n",
      "epoch 24, loss 0.3145, train acc 0.862, test acc 0.792, time 25.8 sec\n",
      "epoch 25, loss 0.3091, train acc 0.864, test acc 0.791, time 25.9 sec\n",
      "epoch 26, loss 0.3043, train acc 0.867, test acc 0.788, time 25.7 sec\n",
      "epoch 27, loss 0.2997, train acc 0.870, test acc 0.792, time 25.8 sec\n",
      "epoch 28, loss 0.2949, train acc 0.873, test acc 0.788, time 25.9 sec\n",
      "epoch 29, loss 0.2901, train acc 0.875, test acc 0.792, time 25.9 sec\n",
      "epoch 30, loss 0.2858, train acc 0.877, test acc 0.789, time 25.8 sec\n",
      "epoch 31, loss 0.2684, train acc 0.888, test acc 0.794, time 25.8 sec\n",
      "epoch 32, loss 0.2650, train acc 0.890, test acc 0.792, time 25.9 sec\n",
      "epoch 33, loss 0.2625, train acc 0.891, test acc 0.794, time 25.7 sec\n",
      "epoch 34, loss 0.2590, train acc 0.893, test acc 0.796, time 25.8 sec\n",
      "epoch 35, loss 0.2562, train acc 0.894, test acc 0.796, time 25.6 sec\n",
      "epoch 36, loss 0.2532, train acc 0.896, test acc 0.795, time 25.8 sec\n",
      "epoch 37, loss 0.2507, train acc 0.897, test acc 0.795, time 26.0 sec\n",
      "epoch 38, loss 0.2472, train acc 0.899, test acc 0.795, time 25.9 sec\n",
      "epoch 39, loss 0.2448, train acc 0.901, test acc 0.794, time 25.8 sec\n",
      "epoch 40, loss 0.2418, train acc 0.902, test acc 0.795, time 25.7 sec\n",
      "epoch 41, loss 0.2327, train acc 0.908, test acc 0.796, time 25.7 sec\n",
      "epoch 42, loss 0.2311, train acc 0.908, test acc 0.796, time 25.9 sec\n",
      "epoch 43, loss 0.2294, train acc 0.909, test acc 0.796, time 25.8 sec\n",
      "epoch 44, loss 0.2278, train acc 0.910, test acc 0.796, time 25.9 sec\n",
      "epoch 45, loss 0.2265, train acc 0.911, test acc 0.796, time 25.9 sec\n",
      "epoch 46, loss 0.2248, train acc 0.912, test acc 0.796, time 25.8 sec\n",
      "epoch 47, loss 0.2230, train acc 0.913, test acc 0.796, time 25.9 sec\n",
      "epoch 48, loss 0.2215, train acc 0.914, test acc 0.795, time 25.7 sec\n",
      "epoch 49, loss 0.2203, train acc 0.914, test acc 0.796, time 25.9 sec\n",
      "epoch 50, loss 0.2191, train acc 0.915, test acc 0.796, time 25.9 sec\n",
      "epoch 51, loss 0.2141, train acc 0.918, test acc 0.797, time 25.9 sec\n",
      "epoch 52, loss 0.2137, train acc 0.918, test acc 0.797, time 25.9 sec\n",
      "epoch 53, loss 0.2121, train acc 0.919, test acc 0.797, time 25.7 sec\n",
      "epoch 54, loss 0.2117, train acc 0.919, test acc 0.797, time 25.8 sec\n",
      "epoch 55, loss 0.2111, train acc 0.919, test acc 0.798, time 25.8 sec\n",
      "epoch 56, loss 0.2108, train acc 0.920, test acc 0.797, time 25.7 sec\n",
      "epoch 57, loss 0.2096, train acc 0.920, test acc 0.797, time 25.8 sec\n",
      "epoch 58, loss 0.2088, train acc 0.921, test acc 0.797, time 25.7 sec\n",
      "epoch 59, loss 0.2078, train acc 0.921, test acc 0.797, time 25.7 sec\n",
      "epoch 60, loss 0.2070, train acc 0.922, test acc 0.797, time 25.8 sec\n",
      "epoch 61, loss 0.2049, train acc 0.923, test acc 0.798, time 25.7 sec\n",
      "epoch 62, loss 0.2046, train acc 0.923, test acc 0.797, time 25.7 sec\n",
      "epoch 63, loss 0.2041, train acc 0.923, test acc 0.797, time 25.8 sec\n",
      "epoch 64, loss 0.2040, train acc 0.923, test acc 0.797, time 25.9 sec\n",
      "epoch 65, loss 0.2030, train acc 0.924, test acc 0.797, time 25.9 sec\n",
      "epoch 66, loss 0.2031, train acc 0.924, test acc 0.798, time 25.9 sec\n",
      "epoch 67, loss 0.2026, train acc 0.924, test acc 0.798, time 25.9 sec\n",
      "epoch 68, loss 0.2023, train acc 0.924, test acc 0.798, time 25.9 sec\n",
      "epoch 69, loss 0.2020, train acc 0.924, test acc 0.797, time 25.8 sec\n",
      "epoch 70, loss 0.2014, train acc 0.925, test acc 0.797, time 25.9 sec\n",
      "epoch 71, loss 0.1998, train acc 0.926, test acc 0.797, time 25.8 sec\n",
      "epoch 72, loss 0.1993, train acc 0.926, test acc 0.797, time 25.8 sec\n",
      "epoch 73, loss 0.1998, train acc 0.926, test acc 0.797, time 25.8 sec\n",
      "epoch 74, loss 0.1996, train acc 0.926, test acc 0.798, time 25.8 sec\n",
      "epoch 75, loss 0.1996, train acc 0.926, test acc 0.797, time 25.9 sec\n",
      "epoch 76, loss 0.1998, train acc 0.926, test acc 0.797, time 25.9 sec\n",
      "epoch 77, loss 0.1992, train acc 0.926, test acc 0.798, time 25.7 sec\n",
      "epoch 78, loss 0.1985, train acc 0.926, test acc 0.798, time 25.8 sec\n",
      "epoch 79, loss 0.1984, train acc 0.927, test acc 0.798, time 25.8 sec\n",
      "epoch 80, loss 0.1981, train acc 0.927, test acc 0.797, time 25.7 sec\n",
      "epoch 81, loss 0.1974, train acc 0.927, test acc 0.798, time 25.8 sec\n",
      "epoch 82, loss 0.1979, train acc 0.926, test acc 0.798, time 25.8 sec\n",
      "epoch 83, loss 0.1979, train acc 0.927, test acc 0.797, time 25.8 sec\n",
      "epoch 84, loss 0.1971, train acc 0.927, test acc 0.798, time 25.8 sec\n",
      "epoch 85, loss 0.1974, train acc 0.927, test acc 0.798, time 25.8 sec\n",
      "epoch 86, loss 0.1971, train acc 0.927, test acc 0.798, time 25.7 sec\n",
      "epoch 87, loss 0.1975, train acc 0.927, test acc 0.797, time 25.8 sec\n",
      "epoch 88, loss 0.1971, train acc 0.927, test acc 0.798, time 25.8 sec\n",
      "epoch 89, loss 0.1970, train acc 0.927, test acc 0.798, time 25.8 sec\n",
      "epoch 90, loss 0.1974, train acc 0.927, test acc 0.797, time 25.6 sec\n",
      "epoch 91, loss 0.1970, train acc 0.928, test acc 0.798, time 25.8 sec\n",
      "epoch 92, loss 0.1967, train acc 0.927, test acc 0.798, time 25.9 sec\n",
      "epoch 93, loss 0.1962, train acc 0.928, test acc 0.798, time 25.9 sec\n",
      "epoch 94, loss 0.1964, train acc 0.928, test acc 0.798, time 25.8 sec\n",
      "epoch 95, loss 0.1960, train acc 0.928, test acc 0.798, time 25.8 sec\n",
      "epoch 96, loss 0.1965, train acc 0.927, test acc 0.797, time 25.8 sec\n",
      "epoch 97, loss 0.1960, train acc 0.928, test acc 0.797, time 25.8 sec\n",
      "epoch 98, loss 0.1963, train acc 0.928, test acc 0.798, time 25.8 sec\n",
      "epoch 99, loss 0.1965, train acc 0.928, test acc 0.798, time 25.9 sec\n",
      "epoch 100, loss 0.1960, train acc 0.928, test acc 0.798, time 25.7 sec\n"
     ]
    }
   ],
   "source": [
    "train(train_iter, test_iter, net, gloss.SoftmaxCrossEntropyLoss(), trainer, ctx, num_epochs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.HybridSequential()\n",
    "net.add(nn.Dense(512, activation='relu'))\n",
    "net.add(gluon.nn.Dropout(0.25))\n",
    "net.add(nn.Dense(2))\n",
    "#     net.initialize(init.Normal(sigma=0.01))\n",
    "net.initialize(init.Xavier(), ctx=ctx)\n",
    "net.hybridize()\n",
    "\n",
    "# net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "batch_size = 64\n",
    "train_set = gdata.ArrayDataset(X_train, nd.array(y_train))\n",
    "test_set = gdata.ArrayDataset(X_test, nd.array(y_test))\n",
    "train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = gdata.DataLoader(test_set, batch_size)\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1, 'wd': 0.95})\n",
    "num_epochs = 100\n",
    "# d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "#               None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.6932, train acc 0.500, test acc 0.506, time 25.8 sec\n",
      "epoch 2, loss 0.6932, train acc 0.500, test acc 0.506, time 25.8 sec\n",
      "epoch 3, loss 0.6932, train acc 0.501, test acc 0.506, time 25.9 sec\n",
      "epoch 4, loss 0.6932, train acc 0.500, test acc 0.494, time 26.0 sec\n",
      "epoch 5, loss 0.6932, train acc 0.500, test acc 0.506, time 25.8 sec\n",
      "epoch 6, loss 0.6932, train acc 0.502, test acc 0.506, time 25.9 sec\n",
      "epoch 7, loss 0.6932, train acc 0.500, test acc 0.506, time 25.9 sec\n",
      "epoch 8, loss 0.6932, train acc 0.502, test acc 0.506, time 25.8 sec\n",
      "epoch 9, loss 0.6932, train acc 0.500, test acc 0.506, time 25.9 sec\n",
      "epoch 10, loss 0.6932, train acc 0.501, test acc 0.506, time 25.8 sec\n",
      "epoch 11, loss 0.6932, train acc 0.500, test acc 0.494, time 25.8 sec\n",
      "epoch 12, loss 0.6931, train acc 0.502, test acc 0.506, time 25.8 sec\n",
      "epoch 13, loss 0.6932, train acc 0.501, test acc 0.506, time 25.8 sec\n",
      "epoch 14, loss 0.6932, train acc 0.502, test acc 0.506, time 25.9 sec\n",
      "epoch 15, loss 0.6932, train acc 0.501, test acc 0.506, time 25.8 sec\n",
      "epoch 16, loss 0.6932, train acc 0.501, test acc 0.494, time 25.8 sec\n",
      "epoch 17, loss 0.6932, train acc 0.503, test acc 0.506, time 25.8 sec\n",
      "epoch 18, loss 0.6932, train acc 0.501, test acc 0.506, time 25.8 sec\n",
      "epoch 19, loss 0.6932, train acc 0.500, test acc 0.494, time 25.8 sec\n",
      "epoch 20, loss 0.6932, train acc 0.501, test acc 0.494, time 25.8 sec\n",
      "epoch 21, loss 0.6931, train acc 0.502, test acc 0.506, time 25.8 sec\n",
      "epoch 22, loss 0.6931, train acc 0.502, test acc 0.506, time 25.8 sec\n",
      "epoch 23, loss 0.6931, train acc 0.502, test acc 0.494, time 25.9 sec\n",
      "epoch 24, loss 0.6931, train acc 0.502, test acc 0.494, time 25.9 sec\n",
      "epoch 25, loss 0.6932, train acc 0.501, test acc 0.494, time 26.0 sec\n",
      "epoch 26, loss 0.6931, train acc 0.502, test acc 0.494, time 25.8 sec\n",
      "epoch 27, loss 0.6931, train acc 0.502, test acc 0.506, time 25.9 sec\n",
      "epoch 28, loss 0.6931, train acc 0.502, test acc 0.506, time 25.9 sec\n",
      "epoch 29, loss 0.6931, train acc 0.502, test acc 0.506, time 25.8 sec\n",
      "epoch 30, loss 0.6932, train acc 0.501, test acc 0.506, time 25.8 sec\n",
      "epoch 31, loss 0.6931, train acc 0.502, test acc 0.506, time 25.9 sec\n",
      "epoch 32, loss 0.6931, train acc 0.502, test acc 0.494, time 25.7 sec\n",
      "epoch 33, loss 0.6931, train acc 0.503, test acc 0.494, time 26.0 sec\n",
      "epoch 34, loss 0.6931, train acc 0.502, test acc 0.506, time 25.9 sec\n",
      "epoch 35, loss 0.6931, train acc 0.501, test acc 0.506, time 25.8 sec\n",
      "epoch 36, loss 0.6931, train acc 0.503, test acc 0.506, time 25.8 sec\n",
      "epoch 37, loss 0.6931, train acc 0.503, test acc 0.506, time 25.8 sec\n",
      "epoch 38, loss 0.6931, train acc 0.503, test acc 0.506, time 25.8 sec\n",
      "epoch 39, loss 0.6931, train acc 0.502, test acc 0.506, time 25.8 sec\n",
      "epoch 40, loss 0.6931, train acc 0.503, test acc 0.506, time 25.8 sec\n",
      "epoch 41, loss 0.6931, train acc 0.503, test acc 0.506, time 26.0 sec\n",
      "epoch 42, loss 0.6931, train acc 0.502, test acc 0.506, time 25.8 sec\n",
      "epoch 43, loss 0.6931, train acc 0.503, test acc 0.506, time 25.9 sec\n",
      "epoch 44, loss 0.6931, train acc 0.503, test acc 0.506, time 25.8 sec\n",
      "epoch 45, loss 0.6931, train acc 0.503, test acc 0.506, time 25.7 sec\n",
      "epoch 46, loss 0.6931, train acc 0.503, test acc 0.506, time 25.8 sec\n",
      "epoch 47, loss 0.6931, train acc 0.503, test acc 0.506, time 25.9 sec\n",
      "epoch 48, loss 0.6931, train acc 0.502, test acc 0.506, time 25.8 sec\n",
      "epoch 49, loss 0.6931, train acc 0.503, test acc 0.506, time 25.9 sec\n",
      "epoch 50, loss 0.6931, train acc 0.503, test acc 0.494, time 25.8 sec\n",
      "epoch 51, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 52, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 53, loss 0.6931, train acc 0.504, test acc 0.494, time 25.8 sec\n",
      "epoch 54, loss 0.6931, train acc 0.504, test acc 0.506, time 25.7 sec\n",
      "epoch 55, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 56, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 57, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 58, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 59, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 60, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 61, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 62, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 63, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 64, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 65, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 66, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 67, loss 0.6931, train acc 0.504, test acc 0.506, time 25.6 sec\n",
      "epoch 68, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 69, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 70, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 71, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 72, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 73, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 74, loss 0.6931, train acc 0.504, test acc 0.506, time 26.0 sec\n",
      "epoch 75, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 76, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 77, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 78, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 79, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 80, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 81, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 82, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 83, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 84, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 85, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 86, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 87, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 88, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 89, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 90, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 91, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 92, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 93, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 94, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 95, loss 0.6931, train acc 0.504, test acc 0.506, time 26.1 sec\n",
      "epoch 96, loss 0.6931, train acc 0.504, test acc 0.506, time 25.9 sec\n",
      "epoch 97, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 98, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 99, loss 0.6931, train acc 0.504, test acc 0.506, time 25.8 sec\n",
      "epoch 100, loss 0.6931, train acc 0.504, test acc 0.506, time 26.0 sec\n"
     ]
    }
   ],
   "source": [
    "train(train_iter, test_iter, net, gloss.SoftmaxCrossEntropyLoss(), trainer, ctx, num_epochs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.HybridSequential()\n",
    "net.add(nn.Dense(512, activation='relu'))\n",
    "net.add(gluon.nn.Dropout(0.25))\n",
    "net.add(nn.Dense(2))\n",
    "#     net.initialize(init.Normal(sigma=0.01))\n",
    "net.initialize(init.Xavier(), ctx=ctx)\n",
    "net.hybridize()\n",
    "\n",
    "# net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "batch_size = 64\n",
    "train_set = gdata.ArrayDataset(X_train, nd.array(y_train))\n",
    "test_set = gdata.ArrayDataset(X_test, nd.array(y_test))\n",
    "train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = gdata.DataLoader(test_set, batch_size)\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1, 'wd': 0.001})\n",
    "num_epochs = 100\n",
    "# d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "#               None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.5865, train acc 0.681, test acc 0.672, time 25.7 sec\n",
      "epoch 2, loss 0.5534, train acc 0.708, test acc 0.710, time 25.7 sec\n",
      "epoch 3, loss 0.5426, train acc 0.717, test acc 0.717, time 25.6 sec\n",
      "epoch 4, loss 0.5370, train acc 0.721, test acc 0.735, time 25.6 sec\n",
      "epoch 5, loss 0.5350, train acc 0.722, test acc 0.731, time 25.8 sec\n",
      "epoch 6, loss 0.5330, train acc 0.724, test acc 0.736, time 25.7 sec\n",
      "epoch 7, loss 0.5317, train acc 0.725, test acc 0.738, time 25.7 sec\n",
      "epoch 8, loss 0.5311, train acc 0.725, test acc 0.736, time 25.7 sec\n",
      "epoch 9, loss 0.5299, train acc 0.726, test acc 0.722, time 25.6 sec\n",
      "epoch 10, loss 0.5299, train acc 0.727, test acc 0.668, time 25.6 sec\n",
      "epoch 11, loss 0.5085, train acc 0.742, test acc 0.743, time 25.8 sec\n",
      "epoch 12, loss 0.5078, train acc 0.744, test acc 0.747, time 25.7 sec\n",
      "epoch 13, loss 0.5068, train acc 0.743, test acc 0.744, time 25.8 sec\n",
      "epoch 14, loss 0.5060, train acc 0.744, test acc 0.701, time 25.7 sec\n",
      "epoch 15, loss 0.5058, train acc 0.745, test acc 0.736, time 25.8 sec\n",
      "epoch 16, loss 0.5049, train acc 0.745, test acc 0.741, time 25.7 sec\n",
      "epoch 17, loss 0.5042, train acc 0.746, test acc 0.730, time 25.7 sec\n",
      "epoch 18, loss 0.5035, train acc 0.746, test acc 0.742, time 25.7 sec\n",
      "epoch 19, loss 0.5040, train acc 0.745, test acc 0.744, time 25.8 sec\n",
      "epoch 20, loss 0.5033, train acc 0.746, test acc 0.751, time 25.6 sec\n",
      "epoch 21, loss 0.4876, train acc 0.758, test acc 0.755, time 25.7 sec\n",
      "epoch 22, loss 0.4865, train acc 0.759, test acc 0.743, time 25.8 sec\n",
      "epoch 23, loss 0.4854, train acc 0.759, test acc 0.725, time 25.8 sec\n",
      "epoch 24, loss 0.4849, train acc 0.760, test acc 0.758, time 25.7 sec\n",
      "epoch 25, loss 0.4842, train acc 0.760, test acc 0.757, time 25.8 sec\n",
      "epoch 26, loss 0.4836, train acc 0.761, test acc 0.745, time 25.8 sec\n",
      "epoch 27, loss 0.4831, train acc 0.761, test acc 0.759, time 25.8 sec\n",
      "epoch 28, loss 0.4827, train acc 0.761, test acc 0.753, time 25.7 sec\n",
      "epoch 29, loss 0.4820, train acc 0.762, test acc 0.761, time 25.8 sec\n",
      "epoch 30, loss 0.4814, train acc 0.762, test acc 0.756, time 25.8 sec\n",
      "epoch 31, loss 0.4715, train acc 0.770, test acc 0.762, time 25.8 sec\n",
      "epoch 32, loss 0.4704, train acc 0.771, test acc 0.759, time 25.9 sec\n",
      "epoch 33, loss 0.4699, train acc 0.771, test acc 0.764, time 25.8 sec\n",
      "epoch 34, loss 0.4693, train acc 0.772, test acc 0.762, time 25.7 sec\n",
      "epoch 35, loss 0.4691, train acc 0.772, test acc 0.763, time 25.8 sec\n",
      "epoch 36, loss 0.4683, train acc 0.772, test acc 0.753, time 25.7 sec\n",
      "epoch 37, loss 0.4677, train acc 0.773, test acc 0.765, time 25.7 sec\n",
      "epoch 38, loss 0.4673, train acc 0.773, test acc 0.764, time 25.7 sec\n",
      "epoch 39, loss 0.4673, train acc 0.773, test acc 0.760, time 25.8 sec\n",
      "epoch 40, loss 0.4667, train acc 0.774, test acc 0.763, time 25.7 sec\n",
      "epoch 41, loss 0.4609, train acc 0.778, test acc 0.762, time 25.7 sec\n",
      "epoch 42, loss 0.4604, train acc 0.779, test acc 0.766, time 25.8 sec\n",
      "epoch 43, loss 0.4601, train acc 0.779, test acc 0.765, time 25.7 sec\n",
      "epoch 44, loss 0.4594, train acc 0.780, test acc 0.766, time 25.8 sec\n",
      "epoch 45, loss 0.4594, train acc 0.779, test acc 0.766, time 25.9 sec\n",
      "epoch 46, loss 0.4590, train acc 0.780, test acc 0.768, time 25.6 sec\n",
      "epoch 47, loss 0.4587, train acc 0.780, test acc 0.765, time 25.6 sec\n",
      "epoch 48, loss 0.4584, train acc 0.780, test acc 0.768, time 25.8 sec\n",
      "epoch 49, loss 0.4581, train acc 0.780, test acc 0.767, time 26.1 sec\n",
      "epoch 50, loss 0.4576, train acc 0.781, test acc 0.766, time 25.7 sec\n",
      "epoch 51, loss 0.4545, train acc 0.783, test acc 0.767, time 25.8 sec\n",
      "epoch 52, loss 0.4545, train acc 0.783, test acc 0.768, time 25.7 sec\n",
      "epoch 53, loss 0.4542, train acc 0.783, test acc 0.768, time 25.7 sec\n",
      "epoch 54, loss 0.4538, train acc 0.784, test acc 0.768, time 25.8 sec\n",
      "epoch 55, loss 0.4538, train acc 0.784, test acc 0.766, time 25.8 sec\n",
      "epoch 56, loss 0.4532, train acc 0.784, test acc 0.768, time 25.8 sec\n",
      "epoch 57, loss 0.4532, train acc 0.784, test acc 0.769, time 25.8 sec\n",
      "epoch 58, loss 0.4531, train acc 0.785, test acc 0.767, time 25.8 sec\n",
      "epoch 59, loss 0.4529, train acc 0.785, test acc 0.767, time 25.7 sec\n",
      "epoch 60, loss 0.4525, train acc 0.785, test acc 0.767, time 25.7 sec\n",
      "epoch 61, loss 0.4511, train acc 0.786, test acc 0.768, time 25.8 sec\n",
      "epoch 62, loss 0.4511, train acc 0.786, test acc 0.769, time 25.8 sec\n",
      "epoch 63, loss 0.4510, train acc 0.786, test acc 0.769, time 25.9 sec\n",
      "epoch 64, loss 0.4507, train acc 0.786, test acc 0.769, time 25.9 sec\n",
      "epoch 65, loss 0.4506, train acc 0.787, test acc 0.769, time 25.8 sec\n",
      "epoch 66, loss 0.4505, train acc 0.787, test acc 0.770, time 25.7 sec\n",
      "epoch 67, loss 0.4504, train acc 0.786, test acc 0.769, time 25.8 sec\n",
      "epoch 68, loss 0.4501, train acc 0.787, test acc 0.769, time 25.8 sec\n",
      "epoch 69, loss 0.4499, train acc 0.787, test acc 0.769, time 25.9 sec\n",
      "epoch 70, loss 0.4499, train acc 0.787, test acc 0.768, time 25.7 sec\n",
      "epoch 71, loss 0.4494, train acc 0.787, test acc 0.769, time 25.8 sec\n",
      "epoch 72, loss 0.4493, train acc 0.787, test acc 0.769, time 25.8 sec\n",
      "epoch 73, loss 0.4491, train acc 0.788, test acc 0.769, time 25.8 sec\n",
      "epoch 74, loss 0.4490, train acc 0.788, test acc 0.769, time 25.8 sec\n",
      "epoch 75, loss 0.4490, train acc 0.788, test acc 0.770, time 25.7 sec\n",
      "epoch 76, loss 0.4488, train acc 0.788, test acc 0.769, time 25.8 sec\n",
      "epoch 77, loss 0.4488, train acc 0.788, test acc 0.769, time 25.7 sec\n",
      "epoch 78, loss 0.4489, train acc 0.788, test acc 0.769, time 25.8 sec\n",
      "epoch 79, loss 0.4488, train acc 0.788, test acc 0.769, time 25.7 sec\n",
      "epoch 80, loss 0.4487, train acc 0.788, test acc 0.770, time 25.7 sec\n",
      "epoch 81, loss 0.4481, train acc 0.788, test acc 0.769, time 25.9 sec\n",
      "epoch 82, loss 0.4481, train acc 0.789, test acc 0.770, time 25.7 sec\n",
      "epoch 83, loss 0.4481, train acc 0.789, test acc 0.770, time 25.7 sec\n",
      "epoch 84, loss 0.4481, train acc 0.789, test acc 0.770, time 25.7 sec\n",
      "epoch 85, loss 0.4482, train acc 0.789, test acc 0.770, time 25.7 sec\n",
      "epoch 86, loss 0.4482, train acc 0.788, test acc 0.769, time 25.8 sec\n",
      "epoch 87, loss 0.4480, train acc 0.789, test acc 0.770, time 25.7 sec\n",
      "epoch 88, loss 0.4481, train acc 0.788, test acc 0.770, time 25.7 sec\n",
      "epoch 89, loss 0.4482, train acc 0.789, test acc 0.770, time 25.8 sec\n",
      "epoch 90, loss 0.4481, train acc 0.788, test acc 0.769, time 25.8 sec\n",
      "epoch 91, loss 0.4479, train acc 0.789, test acc 0.770, time 25.7 sec\n",
      "epoch 92, loss 0.4478, train acc 0.789, test acc 0.770, time 25.8 sec\n",
      "epoch 93, loss 0.4478, train acc 0.789, test acc 0.770, time 25.6 sec\n",
      "epoch 94, loss 0.4477, train acc 0.789, test acc 0.769, time 25.7 sec\n",
      "epoch 95, loss 0.4479, train acc 0.789, test acc 0.769, time 25.8 sec\n",
      "epoch 96, loss 0.4479, train acc 0.789, test acc 0.770, time 25.7 sec\n",
      "epoch 97, loss 0.4477, train acc 0.789, test acc 0.770, time 25.8 sec\n",
      "epoch 98, loss 0.4476, train acc 0.789, test acc 0.770, time 25.8 sec\n",
      "epoch 99, loss 0.4480, train acc 0.789, test acc 0.770, time 25.8 sec\n",
      "epoch 100, loss 0.4476, train acc 0.789, test acc 0.770, time 25.8 sec\n"
     ]
    }
   ],
   "source": [
    "train(train_iter, test_iter, net, gloss.SoftmaxCrossEntropyLoss(), trainer, ctx, num_epochs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs, lr_period):\n",
    "    \"\"\"Train and evaluate a model.\"\"\"\n",
    "    print('training on', ctx)\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, m, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "             trainer.set_learning_rate(trainer.learning_rate * .9)\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            Xs, ys, batch_size = _get_batch(batch, ctx)\n",
    "            ls = []\n",
    "            with autograd.record():\n",
    "                y_hats = [net(X) for X in Xs]\n",
    "                ls = [loss(y_hat, y) for y_hat, y in zip(y_hats, ys)]\n",
    "            for l in ls:\n",
    "                l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_l_sum += sum([l.sum().asscalar() for l in ls])\n",
    "            n += sum([l.size for l in ls])\n",
    "            train_acc_sum += sum([(y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "                                  for y_hat, y in zip(y_hats, ys)])\n",
    "            m += sum([y.size for y in ys])\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / m, test_acc,\n",
    "                 time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.HybridSequential()\n",
    "net.add(nn.Dense(2048, activation='relu'))\n",
    "net.add(gluon.nn.Dropout(0.25))\n",
    "net.add(nn.Dense(512, activation='relu'))\n",
    "net.add(nn.Dense(2))\n",
    "#     net.initialize(init.Normal(sigma=0.01))\n",
    "net.initialize(init.Xavier(), ctx=ctx)\n",
    "net.hybridize()\n",
    "\n",
    "# net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "batch_size = 64\n",
    "train_set = gdata.ArrayDataset(X_train, nd.array(y_train))\n",
    "test_set = gdata.ArrayDataset(X_test, nd.array(y_test))\n",
    "train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = gdata.DataLoader(test_set, batch_size)\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1, 'wd': 0.001})\n",
    "num_epochs = 100\n",
    "# d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "#               None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.5867, train acc 0.681, test acc 0.661, time 29.8 sec\n",
      "epoch 2, loss 0.5526, train acc 0.708, test acc 0.732, time 29.1 sec\n",
      "epoch 3, loss 0.5400, train acc 0.718, test acc 0.735, time 29.1 sec\n",
      "epoch 4, loss 0.5345, train acc 0.722, test acc 0.715, time 28.9 sec\n",
      "epoch 5, loss 0.5310, train acc 0.725, test acc 0.731, time 28.9 sec\n",
      "epoch 6, loss 0.5281, train acc 0.727, test acc 0.745, time 29.1 sec\n",
      "epoch 7, loss 0.5264, train acc 0.727, test acc 0.746, time 29.3 sec\n",
      "epoch 8, loss 0.5257, train acc 0.728, test acc 0.740, time 29.4 sec\n",
      "epoch 9, loss 0.5247, train acc 0.730, test acc 0.748, time 29.0 sec\n",
      "epoch 10, loss 0.5234, train acc 0.729, test acc 0.741, time 29.2 sec\n",
      "epoch 11, loss 0.5190, train acc 0.733, test acc 0.741, time 29.4 sec\n",
      "epoch 12, loss 0.5182, train acc 0.734, test acc 0.744, time 29.4 sec\n",
      "epoch 13, loss 0.5180, train acc 0.733, test acc 0.719, time 29.5 sec\n",
      "epoch 14, loss 0.5165, train acc 0.735, test acc 0.744, time 29.2 sec\n",
      "epoch 15, loss 0.5170, train acc 0.735, test acc 0.721, time 29.2 sec\n",
      "epoch 16, loss 0.5166, train acc 0.735, test acc 0.729, time 29.6 sec\n",
      "epoch 17, loss 0.5163, train acc 0.736, test acc 0.743, time 29.3 sec\n",
      "epoch 18, loss 0.5156, train acc 0.735, test acc 0.639, time 29.1 sec\n",
      "epoch 19, loss 0.5155, train acc 0.736, test acc 0.677, time 29.5 sec\n",
      "epoch 20, loss 0.5153, train acc 0.735, test acc 0.750, time 29.8 sec\n"
     ]
    }
   ],
   "source": [
    "train(train_iter, test_iter, net, gloss.SoftmaxCrossEntropyLoss(), trainer, ctx, num_epochs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
